{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants\n",
    "IMG_SZ = 100\n",
    "TRAIN_NEW_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the linear size of the output layer of a convolutional layer\n",
    "def conv2d_out_sz(in_size, kernel_size, pool_size, padding=0, stride=1):\n",
    "    return ((in_size - kernel_size + 2*padding)/stride + 1)/pool_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5184"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks = 5 # kernel size\n",
    "ps = 2 # pool size\n",
    "out_chan = 64 # number of output channels \n",
    "\n",
    "os = conv2d_out_sz(IMG_SZ,ks,ps) # outputs to 2nd conv2d layer\n",
    "os = conv2d_out_sz(os,ks,ps) # output size of 2nd conv2d layer\n",
    "os = conv2d_out_sz(os,ks,ps) # output size 3\n",
    "\n",
    "fcin = int((int(os)**2)*out_chan)\n",
    "fcin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WildfireModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel_size = 5\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,  out_channels=16,  kernel_size=self.kernel_size)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32,  kernel_size=self.kernel_size)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=self.kernel_size)\n",
    "\n",
    "        self.fc1 = nn.Linear(fcin, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 8)\n",
    "        self.fc5 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.leaky_relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.leaky_relu(self.conv2(x)), (2,2))\n",
    "        x = F.max_pool2d(F.leaky_relu(self.conv3(x)), (2,2))\n",
    "        \n",
    "        x = x.view(-1, fcin) # flatten to input to the linear layer\n",
    "        \n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = F.leaky_relu(self.fc4(x))\n",
    "        x = F.leaky_relu(self.fc5(x))\n",
    "                    \n",
    "        x = F.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((torch.Size([514, 3, 100, 100]), torch.Size([514])),\n",
       " (torch.Size([58, 3, 100, 100]), torch.Size([58])))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the balanced data array to train and test on\n",
    "data = np.load(\"balanced_data.npy\", allow_pickle=True)\n",
    "extra_data = np.load(\"extra_data.npy\", allow_pickle=True)\n",
    "\n",
    "# format x the way torch wants to see it\n",
    "x = torch.tensor([d[0] for d in data])\n",
    "x = (x/255.0).view(-1,3,100,100)\n",
    "# and the extra x\n",
    "extra_x = torch.tensor([ed[0] for ed in extra_data])\n",
    "extra_x = (extra_x/255.0).view(-1,3,100,100)\n",
    "\n",
    "# format y the way torch wants to see it\n",
    "y = torch.tensor([float(d[1]) for d in data])\n",
    "# and the extra y\n",
    "extra_y = torch.tensor([float(ed[1]) for ed in extra_data])\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.1)\n",
    "(train_x.shape, train_y.shape), (test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(test_x, test_y, wfm):\n",
    "    correct = []\n",
    "    incorrect = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_x)):\n",
    "            real_class = test_y[i]\n",
    "            output = wfm(test_x[i:i+1])[0] # returns a list, grab the 0th \n",
    "            predicted_class = np.round(output.detach())\n",
    "            if predicted_class == real_class:\n",
    "                correct += [i]\n",
    "            else:\n",
    "                incorrect += [i]\n",
    "                \n",
    "    accuracy = round(len(correct)/(len(correct) + len(incorrect)), 3)\n",
    "                \n",
    "    return (accuracy, correct, incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounded buffer class, keeps track of 'size' most recent insertions\n",
    "class BoundedNumericList():\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.nums = []\n",
    "        self.next_insertion = 0\n",
    "        \n",
    "    def insert(self, item):\n",
    "        if not isinstance(item, (int, float, complex)) or isinstance(item, bool):\n",
    "            return False\n",
    "        if len(self.nums) < self.size:\n",
    "            self.nums += [item]\n",
    "        else:\n",
    "            self.nums[self.next_insertion % self.size] = item\n",
    "        self.next_insertion += 1\n",
    "        return True\n",
    "\n",
    "    def average(self):\n",
    "        if len(self.nums) == 0: return None\n",
    "        return sum(self.nums) / len(self.nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Loss: 0.693611204624176. Rolling Accuracy: 0.534\n",
      "Saving model at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type WildfireModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Loss: 0.6936047673225403. Rolling Accuracy: 0.534\n",
      "Epoch: 2. Loss: 0.693601131439209. Rolling Accuracy: 0.534\n",
      "Epoch: 3. Loss: 0.6935982704162598. Rolling Accuracy: 0.534\n",
      "Epoch: 4. Loss: 0.6935939788818359. Rolling Accuracy: 0.534\n",
      "Epoch: 5. Loss: 0.6935860514640808. Rolling Accuracy: 0.534\n",
      "Epoch: 6. Loss: 0.6935043334960938. Rolling Accuracy: 0.534\n",
      "Epoch: 7. Loss: 0.6934460997581482. Rolling Accuracy: 0.534\n",
      "Epoch: 8. Loss: 0.6923684477806091. Rolling Accuracy: 0.564\n",
      "Saving model at epoch 8.\n",
      "Epoch: 9. Loss: 0.6572192311286926. Rolling Accuracy: 0.577\n",
      "Saving model at epoch 9.\n",
      "Epoch: 10. Loss: 0.6141074299812317. Rolling Accuracy: 0.595\n",
      "Saving model at epoch 10.\n",
      "Epoch: 11. Loss: 0.5698967576026917. Rolling Accuracy: 0.612\n",
      "Saving model at epoch 11.\n",
      "Epoch: 12. Loss: 0.5315399765968323. Rolling Accuracy: 0.607\n",
      "Epoch: 13. Loss: 0.49154940247535706. Rolling Accuracy: 0.642\n",
      "Saving model at epoch 13.\n",
      "Epoch: 14. Loss: 0.4753129780292511. Rolling Accuracy: 0.664\n",
      "Saving model at epoch 14.\n",
      "Epoch: 15. Loss: 0.45105043053627014. Rolling Accuracy: 0.694\n",
      "Saving model at epoch 15.\n",
      "Epoch: 16. Loss: 0.43627282977104187. Rolling Accuracy: 0.716\n",
      "Saving model at epoch 16.\n",
      "Epoch: 17. Loss: 0.4261579215526581. Rolling Accuracy: 0.707\n",
      "Epoch: 18. Loss: 0.41268157958984375. Rolling Accuracy: 0.716\n",
      "Epoch: 19. Loss: 0.41427353024482727. Rolling Accuracy: 0.707\n",
      "Epoch: 20. Loss: 0.41765928268432617. Rolling Accuracy: 0.699\n",
      "Epoch: 21. Loss: 0.3836826682090759. Rolling Accuracy: 0.707\n",
      "Epoch: 22. Loss: 0.3920428454875946. Rolling Accuracy: 0.699\n",
      "Epoch: 23. Loss: 0.38019633293151855. Rolling Accuracy: 0.703\n",
      "Epoch: 24. Loss: 0.36493179202079773. Rolling Accuracy: 0.707\n",
      "Epoch: 25. Loss: 0.37869900465011597. Rolling Accuracy: 0.716\n",
      "Saving model at epoch 25.\n",
      "Epoch: 26. Loss: 0.3585343360900879. Rolling Accuracy: 0.733\n",
      "Saving model at epoch 26.\n",
      "Epoch: 27. Loss: 0.3384806513786316. Rolling Accuracy: 0.755\n",
      "Saving model at epoch 27.\n",
      "Epoch: 28. Loss: 0.34628695249557495. Rolling Accuracy: 0.78\n",
      "Saving model at epoch 28.\n",
      "Epoch: 29. Loss: 0.3226347863674164. Rolling Accuracy: 0.793\n",
      "Saving model at epoch 29.\n",
      "Epoch: 30. Loss: 0.30123281478881836. Rolling Accuracy: 0.819\n",
      "Saving model at epoch 30.\n",
      "Epoch: 31. Loss: 0.32861486077308655. Rolling Accuracy: 0.823\n",
      "Saving model at epoch 31.\n",
      "Epoch: 32. Loss: 0.2996048331260681. Rolling Accuracy: 0.828\n",
      "Saving model at epoch 32.\n",
      "Epoch: 33. Loss: 0.26823902130126953. Rolling Accuracy: 0.832\n",
      "Saving model at epoch 33.\n",
      "Epoch: 34. Loss: 0.2645712196826935. Rolling Accuracy: 0.828\n",
      "Epoch: 35. Loss: 0.264146089553833. Rolling Accuracy: 0.837\n",
      "Saving model at epoch 35.\n",
      "Epoch: 36. Loss: 0.24864518642425537. Rolling Accuracy: 0.837\n",
      "Epoch: 37. Loss: 0.2551513910293579. Rolling Accuracy: 0.837\n",
      "Epoch: 38. Loss: 0.23850250244140625. Rolling Accuracy: 0.837\n",
      "Epoch: 39. Loss: 0.23507246375083923. Rolling Accuracy: 0.841\n",
      "Saving model at epoch 39.\n",
      "Epoch: 40. Loss: 0.21962520480155945. Rolling Accuracy: 0.845\n",
      "Saving model at epoch 40.\n",
      "Epoch: 41. Loss: 0.2323932945728302. Rolling Accuracy: 0.845\n",
      "Epoch: 42. Loss: 0.20749667286872864. Rolling Accuracy: 0.841\n",
      "Epoch: 43. Loss: 0.21645720303058624. Rolling Accuracy: 0.837\n",
      "Epoch: 44. Loss: 0.21795400977134705. Rolling Accuracy: 0.832\n",
      "Epoch: 45. Loss: 0.19820062816143036. Rolling Accuracy: 0.837\n",
      "Epoch: 46. Loss: 0.2408076822757721. Rolling Accuracy: 0.841\n",
      "Epoch: 47. Loss: 0.21270641684532166. Rolling Accuracy: 0.836\n",
      "Epoch: 48. Loss: 0.20619891583919525. Rolling Accuracy: 0.845\n",
      "Epoch: 49. Loss: 0.19411492347717285. Rolling Accuracy: 0.849\n",
      "Saving model at epoch 49.\n",
      "Epoch: 50. Loss: 0.22640621662139893. Rolling Accuracy: 0.849\n",
      "Epoch: 51. Loss: 0.18690726161003113. Rolling Accuracy: 0.849\n",
      "Epoch: 52. Loss: 0.2229427844285965. Rolling Accuracy: 0.841\n",
      "Epoch: 53. Loss: 0.1888267993927002. Rolling Accuracy: 0.841\n",
      "Epoch: 54. Loss: 0.24850474298000336. Rolling Accuracy: 0.841\n",
      "Epoch: 55. Loss: 0.19402292370796204. Rolling Accuracy: 0.841\n",
      "Epoch: 56. Loss: 0.20198853313922882. Rolling Accuracy: 0.845\n",
      "Epoch: 57. Loss: 0.18398407101631165. Rolling Accuracy: 0.845\n",
      "Epoch: 58. Loss: 0.2469717264175415. Rolling Accuracy: 0.841\n",
      "Epoch: 59. Loss: 0.1815611571073532. Rolling Accuracy: 0.841\n",
      "Epoch: 60. Loss: 0.20460078120231628. Rolling Accuracy: 0.841\n",
      "Epoch: 61. Loss: 0.18119098246097565. Rolling Accuracy: 0.849\n",
      "Saving model at epoch 61.\n",
      "Epoch: 62. Loss: 0.24774329364299774. Rolling Accuracy: 0.836\n",
      "Epoch: 63. Loss: 0.2519625723361969. Rolling Accuracy: 0.849\n",
      "Epoch: 64. Loss: 0.27097001671791077. Rolling Accuracy: 0.815\n",
      "Epoch: 65. Loss: 0.19064554572105408. Rolling Accuracy: 0.81\n",
      "Epoch: 66. Loss: 0.18971557915210724. Rolling Accuracy: 0.828\n",
      "Epoch: 67. Loss: 0.19534794986248016. Rolling Accuracy: 0.81\n",
      "Epoch: 68. Loss: 0.17339038848876953. Rolling Accuracy: 0.858\n",
      "Saving model at epoch 68.\n",
      "Epoch: 69. Loss: 0.18445083498954773. Rolling Accuracy: 0.841\n",
      "Epoch: 70. Loss: 0.1638806313276291. Rolling Accuracy: 0.858\n",
      "Epoch: 71. Loss: 0.20127248764038086. Rolling Accuracy: 0.849\n",
      "Epoch: 72. Loss: 0.16527783870697021. Rolling Accuracy: 0.849\n",
      "Epoch: 73. Loss: 0.18345509469509125. Rolling Accuracy: 0.854\n",
      "Epoch: 74. Loss: 0.16626419126987457. Rolling Accuracy: 0.854\n",
      "Epoch: 75. Loss: 0.22956253588199615. Rolling Accuracy: 0.862\n",
      "Saving model at epoch 75.\n",
      "Epoch: 76. Loss: 0.1707163006067276. Rolling Accuracy: 0.862\n",
      "Epoch: 77. Loss: 0.19140954315662384. Rolling Accuracy: 0.858\n",
      "Epoch: 78. Loss: 0.16892285645008087. Rolling Accuracy: 0.845\n",
      "Epoch: 79. Loss: 0.1727672815322876. Rolling Accuracy: 0.845\n",
      "Epoch: 80. Loss: 0.15765003859996796. Rolling Accuracy: 0.845\n",
      "Epoch: 81. Loss: 0.17547869682312012. Rolling Accuracy: 0.836\n",
      "Epoch: 82. Loss: 0.15604476630687714. Rolling Accuracy: 0.845\n",
      "Epoch: 83. Loss: 0.1793881356716156. Rolling Accuracy: 0.832\n",
      "Epoch: 84. Loss: 0.16279959678649902. Rolling Accuracy: 0.824\n",
      "Epoch: 85. Loss: 0.16537098586559296. Rolling Accuracy: 0.828\n",
      "Epoch: 86. Loss: 0.16160239279270172. Rolling Accuracy: 0.81\n",
      "Epoch: 87. Loss: 0.16280682384967804. Rolling Accuracy: 0.81\n",
      "Epoch: 88. Loss: 0.15765248239040375. Rolling Accuracy: 0.797\n",
      "Epoch: 89. Loss: 0.15142443776130676. Rolling Accuracy: 0.806\n",
      "Epoch: 90. Loss: 0.15552295744419098. Rolling Accuracy: 0.802\n",
      "Epoch: 91. Loss: 0.15257704257965088. Rolling Accuracy: 0.819\n",
      "Epoch: 92. Loss: 0.15444199740886688. Rolling Accuracy: 0.819\n",
      "Epoch: 93. Loss: 0.14986269176006317. Rolling Accuracy: 0.828\n",
      "Epoch: 94. Loss: 0.1556934267282486. Rolling Accuracy: 0.832\n",
      "Epoch: 95. Loss: 0.14778950810432434. Rolling Accuracy: 0.832\n",
      "Epoch: 96. Loss: 0.17410419881343842. Rolling Accuracy: 0.823\n",
      "Epoch: 97. Loss: 0.1505100429058075. Rolling Accuracy: 0.815\n",
      "Epoch: 98. Loss: 0.15982599556446075. Rolling Accuracy: 0.819\n",
      "Epoch: 99. Loss: 0.14827583730220795. Rolling Accuracy: 0.832\n",
      "Epoch: 100. Loss: 0.150775745511055. Rolling Accuracy: 0.849\n",
      "Epoch: 101. Loss: 0.18453407287597656. Rolling Accuracy: 0.845\n",
      "Epoch: 102. Loss: 0.1510574072599411. Rolling Accuracy: 0.828\n",
      "Epoch: 103. Loss: 0.15747082233428955. Rolling Accuracy: 0.789\n",
      "Epoch: 104. Loss: 0.14446239173412323. Rolling Accuracy: 0.793\n",
      "Epoch: 105. Loss: 0.3229285776615143. Rolling Accuracy: 0.78\n",
      "Epoch: 106. Loss: 0.14985372126102448. Rolling Accuracy: 0.806\n",
      "Epoch: 107. Loss: 0.1868317425251007. Rolling Accuracy: 0.806\n",
      "Epoch: 108. Loss: 0.1714637726545334. Rolling Accuracy: 0.815\n",
      "Epoch: 109. Loss: 0.21874259412288666. Rolling Accuracy: 0.798\n",
      "Epoch: 110. Loss: 0.16785740852355957. Rolling Accuracy: 0.776\n",
      "Epoch: 111. Loss: 0.1348736733198166. Rolling Accuracy: 0.785\n",
      "Epoch: 112. Loss: 0.13235332071781158. Rolling Accuracy: 0.763\n",
      "Epoch: 113. Loss: 0.13025355339050293. Rolling Accuracy: 0.78\n",
      "Epoch: 114. Loss: 0.12490349262952805. Rolling Accuracy: 0.784\n",
      "Epoch: 115. Loss: 0.12364815920591354. Rolling Accuracy: 0.802\n",
      "Epoch: 116. Loss: 0.27660709619522095. Rolling Accuracy: 0.784\n",
      "Epoch: 117. Loss: 0.1399552971124649. Rolling Accuracy: 0.81\n",
      "Epoch: 118. Loss: 0.17483119666576385. Rolling Accuracy: 0.784\n",
      "Epoch: 119. Loss: 0.1401773989200592. Rolling Accuracy: 0.784\n",
      "Epoch: 120. Loss: 0.12616242468357086. Rolling Accuracy: 0.789\n",
      "Epoch: 121. Loss: 0.11918220669031143. Rolling Accuracy: 0.754\n",
      "Epoch: 122. Loss: 0.12017034739255905. Rolling Accuracy: 0.78\n",
      "Epoch: 123. Loss: 0.14669330418109894. Rolling Accuracy: 0.754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 124. Loss: 0.12733305990695953. Rolling Accuracy: 0.784\n",
      "Epoch: 125. Loss: 0.12367036193609238. Rolling Accuracy: 0.788\n",
      "Epoch: 126. Loss: 0.12471633404493332. Rolling Accuracy: 0.793\n",
      "Epoch: 127. Loss: 0.12367693334817886. Rolling Accuracy: 0.823\n",
      "Epoch: 128. Loss: 0.11546500772237778. Rolling Accuracy: 0.784\n",
      "Epoch: 129. Loss: 0.10786253213882446. Rolling Accuracy: 0.78\n",
      "Epoch: 130. Loss: 0.10478372871875763. Rolling Accuracy: 0.763\n",
      "Epoch: 131. Loss: 0.09846310317516327. Rolling Accuracy: 0.737\n",
      "Epoch: 132. Loss: 0.09582126885652542. Rolling Accuracy: 0.75\n",
      "Epoch: 133. Loss: 0.09249485284090042. Rolling Accuracy: 0.763\n",
      "Epoch: 134. Loss: 0.20422758162021637. Rolling Accuracy: 0.754\n",
      "Epoch: 135. Loss: 0.10763001441955566. Rolling Accuracy: 0.78\n",
      "Epoch: 136. Loss: 0.10526105016469955. Rolling Accuracy: 0.797\n",
      "Epoch: 137. Loss: 0.09014960378408432. Rolling Accuracy: 0.789\n",
      "Epoch: 138. Loss: 0.09227077662944794. Rolling Accuracy: 0.797\n",
      "Epoch: 139. Loss: 0.10138881951570511. Rolling Accuracy: 0.793\n",
      "Epoch: 140. Loss: 0.08915821462869644. Rolling Accuracy: 0.772\n",
      "Epoch: 141. Loss: 0.08556056767702103. Rolling Accuracy: 0.78\n",
      "Epoch: 142. Loss: 0.08016998320817947. Rolling Accuracy: 0.776\n",
      "Epoch: 143. Loss: 0.0860498920083046. Rolling Accuracy: 0.767\n",
      "Epoch: 144. Loss: 0.2888135313987732. Rolling Accuracy: 0.75\n",
      "Epoch: 145. Loss: 0.11978311836719513. Rolling Accuracy: 0.776\n",
      "Epoch: 146. Loss: 0.10084392130374908. Rolling Accuracy: 0.772\n",
      "Epoch: 147. Loss: 0.11194954067468643. Rolling Accuracy: 0.772\n",
      "Epoch: 148. Loss: 0.11002162843942642. Rolling Accuracy: 0.819\n",
      "Epoch: 149. Loss: 0.09062794595956802. Rolling Accuracy: 0.802\n",
      "Epoch: 150. Loss: 0.09180476516485214. Rolling Accuracy: 0.823\n",
      "Epoch: 151. Loss: 0.08397184312343597. Rolling Accuracy: 0.819\n",
      "Epoch: 152. Loss: 0.08972537517547607. Rolling Accuracy: 0.806\n",
      "Epoch: 153. Loss: 0.08373624831438065. Rolling Accuracy: 0.802\n",
      "Epoch: 154. Loss: 0.07766664028167725. Rolling Accuracy: 0.798\n",
      "Epoch: 155. Loss: 0.0962902158498764. Rolling Accuracy: 0.815\n",
      "Epoch: 156. Loss: 0.08378138393163681. Rolling Accuracy: 0.798\n",
      "Epoch: 157. Loss: 0.08156972378492355. Rolling Accuracy: 0.802\n",
      "Epoch: 158. Loss: 0.08494044840335846. Rolling Accuracy: 0.81\n",
      "Epoch: 159. Loss: 0.07733982801437378. Rolling Accuracy: 0.802\n",
      "Epoch: 160. Loss: 0.09574239701032639. Rolling Accuracy: 0.823\n",
      "Epoch: 161. Loss: 0.08679483830928802. Rolling Accuracy: 0.815\n",
      "Epoch: 162. Loss: 0.09224504977464676. Rolling Accuracy: 0.819\n",
      "Epoch: 163. Loss: 0.07681895792484283. Rolling Accuracy: 0.819\n",
      "Epoch: 164. Loss: 0.0804215595126152. Rolling Accuracy: 0.823\n",
      "Epoch: 165. Loss: 0.07940472662448883. Rolling Accuracy: 0.81\n",
      "Epoch: 166. Loss: 0.0819997563958168. Rolling Accuracy: 0.806\n",
      "Epoch: 167. Loss: 0.09278709441423416. Rolling Accuracy: 0.819\n",
      "Epoch: 168. Loss: 0.06836454570293427. Rolling Accuracy: 0.806\n",
      "Epoch: 169. Loss: 0.08463465422391891. Rolling Accuracy: 0.836\n",
      "Epoch: 170. Loss: 0.07282636314630508. Rolling Accuracy: 0.832\n",
      "Epoch: 171. Loss: 0.0652981624007225. Rolling Accuracy: 0.815\n",
      "Epoch: 172. Loss: 0.07348769903182983. Rolling Accuracy: 0.819\n",
      "Epoch: 173. Loss: 0.07160816341638565. Rolling Accuracy: 0.815\n",
      "Epoch: 174. Loss: 0.07886989414691925. Rolling Accuracy: 0.824\n",
      "Epoch: 175. Loss: 0.06052699312567711. Rolling Accuracy: 0.824\n",
      "Epoch: 176. Loss: 0.05535396188497543. Rolling Accuracy: 0.819\n",
      "Epoch: 177. Loss: 0.09901082515716553. Rolling Accuracy: 0.823\n",
      "Epoch: 178. Loss: 0.06167709082365036. Rolling Accuracy: 0.823\n",
      "Epoch: 179. Loss: 0.06928706914186478. Rolling Accuracy: 0.836\n",
      "Epoch: 180. Loss: 0.07327336072921753. Rolling Accuracy: 0.841\n",
      "Epoch: 181. Loss: 0.07273101061582565. Rolling Accuracy: 0.841\n",
      "Epoch: 182. Loss: 0.06444866955280304. Rolling Accuracy: 0.837\n",
      "Epoch: 183. Loss: 0.06239308789372444. Rolling Accuracy: 0.828\n",
      "Epoch: 184. Loss: 0.0791255459189415. Rolling Accuracy: 0.828\n",
      "Epoch: 185. Loss: 0.05177826061844826. Rolling Accuracy: 0.815\n",
      "Epoch: 186. Loss: 0.0577101968228817. Rolling Accuracy: 0.815\n",
      "Epoch: 187. Loss: 0.06405892223119736. Rolling Accuracy: 0.823\n",
      "Epoch: 188. Loss: 0.0683792307972908. Rolling Accuracy: 0.828\n",
      "Epoch: 189. Loss: 0.06055404618382454. Rolling Accuracy: 0.832\n",
      "Epoch: 190. Loss: 0.07633223384618759. Rolling Accuracy: 0.849\n",
      "Epoch: 191. Loss: 0.0571804903447628. Rolling Accuracy: 0.836\n",
      "Epoch: 192. Loss: 0.06383969634771347. Rolling Accuracy: 0.836\n",
      "Epoch: 193. Loss: 0.07946890592575073. Rolling Accuracy: 0.849\n",
      "Epoch: 194. Loss: 0.05624726787209511. Rolling Accuracy: 0.836\n",
      "Epoch: 195. Loss: 0.06355738639831543. Rolling Accuracy: 0.849\n",
      "Epoch: 196. Loss: 0.057579439133405685. Rolling Accuracy: 0.849\n",
      "Epoch: 197. Loss: 0.05949011817574501. Rolling Accuracy: 0.845\n",
      "Epoch: 198. Loss: 0.06006333976984024. Rolling Accuracy: 0.845\n",
      "Epoch: 199. Loss: 0.046172309666872025. Rolling Accuracy: 0.832\n",
      "Epoch: 200. Loss: 0.0712248757481575. Rolling Accuracy: 0.836\n",
      "Epoch: 201. Loss: 0.050766561180353165. Rolling Accuracy: 0.832\n",
      "Epoch: 202. Loss: 0.05181233212351799. Rolling Accuracy: 0.823\n",
      "Epoch: 203. Loss: 0.059902142733335495. Rolling Accuracy: 0.841\n",
      "Epoch: 204. Loss: 0.06424085050821304. Rolling Accuracy: 0.836\n",
      "Epoch: 205. Loss: 0.06200813129544258. Rolling Accuracy: 0.841\n",
      "Epoch: 206. Loss: 0.06404004245996475. Rolling Accuracy: 0.854\n",
      "Epoch: 207. Loss: 0.051234662532806396. Rolling Accuracy: 0.836\n",
      "Epoch: 208. Loss: 0.06867239624261856. Rolling Accuracy: 0.836\n",
      "Epoch: 209. Loss: 0.04731079936027527. Rolling Accuracy: 0.832\n",
      "Epoch: 210. Loss: 0.06505002826452255. Rolling Accuracy: 0.832\n",
      "Epoch: 211. Loss: 0.0546027347445488. Rolling Accuracy: 0.841\n",
      "Epoch: 212. Loss: 0.044209521263837814. Rolling Accuracy: 0.836\n",
      "Epoch: 213. Loss: 0.07471714168787003. Rolling Accuracy: 0.836\n",
      "Epoch: 214. Loss: 0.12452234327793121. Rolling Accuracy: 0.806\n",
      "Epoch: 215. Loss: 0.07516588270664215. Rolling Accuracy: 0.815\n",
      "Epoch: 216. Loss: 0.05009155720472336. Rolling Accuracy: 0.815\n",
      "Epoch: 217. Loss: 0.05281445011496544. Rolling Accuracy: 0.828\n",
      "Epoch: 218. Loss: 0.05837337300181389. Rolling Accuracy: 0.853\n",
      "Epoch: 219. Loss: 0.05218592658638954. Rolling Accuracy: 0.858\n",
      "Epoch: 220. Loss: 0.046455513685941696. Rolling Accuracy: 0.858\n",
      "Epoch: 221. Loss: 0.0606827475130558. Rolling Accuracy: 0.849\n",
      "Epoch: 222. Loss: 0.06249840185046196. Rolling Accuracy: 0.858\n",
      "Epoch: 223. Loss: 0.04362335801124573. Rolling Accuracy: 0.849\n",
      "Epoch: 224. Loss: 0.05708860605955124. Rolling Accuracy: 0.858\n",
      "Epoch: 225. Loss: 0.06363547593355179. Rolling Accuracy: 0.866\n",
      "Saving model at epoch 225.\n",
      "Epoch: 226. Loss: 0.04641013219952583. Rolling Accuracy: 0.858\n",
      "Epoch: 227. Loss: 0.04882894828915596. Rolling Accuracy: 0.866\n",
      "Saving model at epoch 227.\n",
      "Epoch: 228. Loss: 0.05294289067387581. Rolling Accuracy: 0.866\n",
      "Epoch: 229. Loss: 0.05131123214960098. Rolling Accuracy: 0.866\n",
      "Epoch: 230. Loss: 0.04524961858987808. Rolling Accuracy: 0.875\n",
      "Saving model at epoch 230.\n",
      "Epoch: 231. Loss: 0.050539545714855194. Rolling Accuracy: 0.871\n",
      "Epoch: 232. Loss: 0.054654311388731. Rolling Accuracy: 0.879\n",
      "Saving model at epoch 232.\n",
      "Epoch: 233. Loss: 0.05202876776456833. Rolling Accuracy: 0.862\n",
      "Epoch: 234. Loss: 0.05912063270807266. Rolling Accuracy: 0.858\n",
      "Epoch: 235. Loss: 0.03919673338532448. Rolling Accuracy: 0.858\n",
      "Epoch: 236. Loss: 0.060538314282894135. Rolling Accuracy: 0.849\n",
      "Epoch: 237. Loss: 0.042637042701244354. Rolling Accuracy: 0.849\n",
      "Epoch: 238. Loss: 0.045276470482349396. Rolling Accuracy: 0.858\n",
      "Epoch: 239. Loss: 0.04142380878329277. Rolling Accuracy: 0.849\n",
      "Epoch: 240. Loss: 0.0494806133210659. Rolling Accuracy: 0.845\n",
      "Epoch: 241. Loss: 0.040852390229701996. Rolling Accuracy: 0.858\n",
      "Epoch: 242. Loss: 0.045723412185907364. Rolling Accuracy: 0.849\n",
      "Epoch: 243. Loss: 0.03875989466905594. Rolling Accuracy: 0.858\n",
      "Epoch: 244. Loss: 0.04923013225197792. Rolling Accuracy: 0.866\n",
      "Epoch: 245. Loss: 0.04591906815767288. Rolling Accuracy: 0.866\n",
      "Epoch: 246. Loss: 0.03946596756577492. Rolling Accuracy: 0.866\n",
      "Epoch: 247. Loss: 0.03457791730761528. Rolling Accuracy: 0.862\n",
      "Epoch: 248. Loss: 0.0505688451230526. Rolling Accuracy: 0.862\n",
      "Epoch: 249. Loss: 0.036570705473423004. Rolling Accuracy: 0.871\n",
      "Epoch: 250. Loss: 0.04660528525710106. Rolling Accuracy: 0.875\n",
      "Epoch: 251. Loss: 0.04220718517899513. Rolling Accuracy: 0.862\n",
      "Epoch: 252. Loss: 0.06149272248148918. Rolling Accuracy: 0.841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 253. Loss: 0.03802249953150749. Rolling Accuracy: 0.828\n",
      "Epoch: 254. Loss: 0.04003690928220749. Rolling Accuracy: 0.823\n",
      "Epoch: 255. Loss: 0.05978149175643921. Rolling Accuracy: 0.841\n",
      "Epoch: 256. Loss: 0.3902130722999573. Rolling Accuracy: 0.819\n",
      "Epoch: 257. Loss: 0.07612423598766327. Rolling Accuracy: 0.823\n",
      "Epoch: 258. Loss: 0.05957796424627304. Rolling Accuracy: 0.806\n",
      "Epoch: 259. Loss: 0.0499989315867424. Rolling Accuracy: 0.797\n",
      "Epoch: 260. Loss: 0.05417848378419876. Rolling Accuracy: 0.845\n",
      "Epoch: 261. Loss: 0.035605404525995255. Rolling Accuracy: 0.845\n",
      "Epoch: 262. Loss: 0.03921739012002945. Rolling Accuracy: 0.866\n",
      "Epoch: 263. Loss: 0.03606672212481499. Rolling Accuracy: 0.879\n",
      "Epoch: 264. Loss: 0.03756803274154663. Rolling Accuracy: 0.875\n",
      "Epoch: 265. Loss: 0.03382088243961334. Rolling Accuracy: 0.879\n",
      "Epoch: 266. Loss: 0.04644882678985596. Rolling Accuracy: 0.87\n",
      "Epoch: 267. Loss: 0.06394635885953903. Rolling Accuracy: 0.853\n",
      "Epoch: 268. Loss: 0.04874451085925102. Rolling Accuracy: 0.862\n",
      "Epoch: 269. Loss: 0.037401314824819565. Rolling Accuracy: 0.849\n",
      "Epoch: 270. Loss: 0.03454449027776718. Rolling Accuracy: 0.849\n",
      "Epoch: 271. Loss: 0.040107499808073044. Rolling Accuracy: 0.866\n",
      "Epoch: 272. Loss: 0.039219971746206284. Rolling Accuracy: 0.858\n",
      "Epoch: 273. Loss: 0.033657439053058624. Rolling Accuracy: 0.866\n",
      "Epoch: 274. Loss: 0.03362208977341652. Rolling Accuracy: 0.875\n",
      "Epoch: 275. Loss: 0.04183029755949974. Rolling Accuracy: 0.871\n",
      "Epoch: 276. Loss: 0.03141436725854874. Rolling Accuracy: 0.866\n",
      "Epoch: 277. Loss: 0.04458910599350929. Rolling Accuracy: 0.862\n",
      "Epoch: 278. Loss: 0.0327562540769577. Rolling Accuracy: 0.867\n",
      "Epoch: 279. Loss: 0.035741161555051804. Rolling Accuracy: 0.862\n",
      "Epoch: 280. Loss: 0.03233975172042847. Rolling Accuracy: 0.867\n",
      "Epoch: 281. Loss: 0.03580034151673317. Rolling Accuracy: 0.875\n",
      "Epoch: 282. Loss: 0.03128115087747574. Rolling Accuracy: 0.866\n",
      "Epoch: 283. Loss: 0.03370377793908119. Rolling Accuracy: 0.884\n",
      "Saving model at epoch 283.\n",
      "Epoch: 284. Loss: 0.032927628606557846. Rolling Accuracy: 0.884\n",
      "Epoch: 285. Loss: 0.03389519825577736. Rolling Accuracy: 0.884\n",
      "Epoch: 286. Loss: 0.029512258246541023. Rolling Accuracy: 0.888\n",
      "Saving model at epoch 286.\n",
      "Epoch: 287. Loss: 0.03225629776716232. Rolling Accuracy: 0.875\n",
      "Epoch: 288. Loss: 0.029655592516064644. Rolling Accuracy: 0.875\n",
      "Epoch: 289. Loss: 0.032308757305145264. Rolling Accuracy: 0.866\n",
      "Epoch: 290. Loss: 0.02823559381067753. Rolling Accuracy: 0.866\n",
      "Epoch: 291. Loss: 0.030811220407485962. Rolling Accuracy: 0.862\n",
      "Epoch: 292. Loss: 0.030414296314120293. Rolling Accuracy: 0.862\n",
      "Epoch: 293. Loss: 0.026707058772444725. Rolling Accuracy: 0.849\n",
      "Epoch: 294. Loss: 0.033900368958711624. Rolling Accuracy: 0.853\n",
      "Epoch: 295. Loss: 0.027795404195785522. Rolling Accuracy: 0.858\n",
      "Epoch: 296. Loss: 0.026442985981702805. Rolling Accuracy: 0.854\n",
      "Epoch: 297. Loss: 0.030493106693029404. Rolling Accuracy: 0.875\n",
      "Epoch: 298. Loss: 0.024909991770982742. Rolling Accuracy: 0.849\n",
      "Epoch: 299. Loss: 0.03369251638650894. Rolling Accuracy: 0.849\n",
      "Epoch: 300. Loss: 0.028552008792757988. Rolling Accuracy: 0.849\n",
      "Epoch: 301. Loss: 0.03410422056913376. Rolling Accuracy: 0.841\n",
      "Epoch: 302. Loss: 0.024814408272504807. Rolling Accuracy: 0.862\n",
      "Epoch: 303. Loss: 0.0355500727891922. Rolling Accuracy: 0.866\n",
      "Epoch: 304. Loss: 0.025121556594967842. Rolling Accuracy: 0.862\n",
      "Epoch: 305. Loss: 0.02811446413397789. Rolling Accuracy: 0.866\n",
      "Epoch: 306. Loss: 0.027203695848584175. Rolling Accuracy: 0.862\n",
      "Epoch: 307. Loss: 0.027421658858656883. Rolling Accuracy: 0.853\n",
      "Epoch: 308. Loss: 0.025031249970197678. Rolling Accuracy: 0.853\n",
      "Epoch: 309. Loss: 0.02619779482483864. Rolling Accuracy: 0.858\n",
      "Epoch: 310. Loss: 0.02516810968518257. Rolling Accuracy: 0.853\n",
      "Epoch: 311. Loss: 0.028630763292312622. Rolling Accuracy: 0.849\n",
      "Epoch: 312. Loss: 0.022883789613842964. Rolling Accuracy: 0.862\n",
      "Epoch: 313. Loss: 0.03434879705309868. Rolling Accuracy: 0.867\n",
      "Epoch: 314. Loss: 0.02385670877993107. Rolling Accuracy: 0.871\n",
      "Epoch: 315. Loss: 0.030637016519904137. Rolling Accuracy: 0.875\n",
      "Epoch: 316. Loss: 0.02478829026222229. Rolling Accuracy: 0.871\n",
      "Epoch: 317. Loss: 0.023517649620771408. Rolling Accuracy: 0.862\n",
      "Epoch: 318. Loss: 0.024998825043439865. Rolling Accuracy: 0.866\n",
      "Epoch: 319. Loss: 0.026618240401148796. Rolling Accuracy: 0.871\n",
      "Epoch: 320. Loss: 0.026147276163101196. Rolling Accuracy: 0.862\n",
      "Epoch: 321. Loss: 0.029027065262198448. Rolling Accuracy: 0.858\n",
      "Epoch: 322. Loss: 0.03773779049515724. Rolling Accuracy: 0.841\n",
      "Epoch: 323. Loss: 0.020463675260543823. Rolling Accuracy: 0.836\n",
      "Epoch: 324. Loss: 0.025414345785975456. Rolling Accuracy: 0.849\n",
      "Epoch: 325. Loss: 0.029632389545440674. Rolling Accuracy: 0.853\n",
      "Epoch: 326. Loss: 0.025384094566106796. Rolling Accuracy: 0.867\n",
      "Epoch: 327. Loss: 0.02349081076681614. Rolling Accuracy: 0.875\n",
      "Epoch: 328. Loss: 0.026227356866002083. Rolling Accuracy: 0.866\n",
      "Epoch: 329. Loss: 0.02650688961148262. Rolling Accuracy: 0.871\n",
      "Epoch: 330. Loss: 0.0253666490316391. Rolling Accuracy: 0.862\n",
      "Epoch: 331. Loss: 0.02339891903102398. Rolling Accuracy: 0.862\n",
      "Epoch: 332. Loss: 0.037331126630306244. Rolling Accuracy: 0.862\n",
      "Epoch: 333. Loss: 0.05728680267930031. Rolling Accuracy: 0.862\n",
      "Epoch: 334. Loss: 0.03664952516555786. Rolling Accuracy: 0.879\n",
      "Epoch: 335. Loss: 0.025592239573597908. Rolling Accuracy: 0.875\n",
      "Epoch: 336. Loss: 0.029144424945116043. Rolling Accuracy: 0.879\n",
      "Epoch: 337. Loss: 0.024706972762942314. Rolling Accuracy: 0.875\n",
      "Epoch: 338. Loss: 0.022438842803239822. Rolling Accuracy: 0.871\n",
      "Epoch: 339. Loss: 0.03129321709275246. Rolling Accuracy: 0.866\n",
      "Epoch: 340. Loss: 0.029207369312644005. Rolling Accuracy: 0.866\n",
      "Epoch: 341. Loss: 0.02869291603565216. Rolling Accuracy: 0.871\n",
      "Epoch: 342. Loss: 0.03076355718076229. Rolling Accuracy: 0.871\n",
      "Epoch: 343. Loss: 0.027738826349377632. Rolling Accuracy: 0.871\n",
      "Epoch: 344. Loss: 0.024769941344857216. Rolling Accuracy: 0.875\n",
      "Epoch: 345. Loss: 0.02582629956305027. Rolling Accuracy: 0.871\n",
      "Epoch: 346. Loss: 0.03030574508011341. Rolling Accuracy: 0.867\n",
      "Epoch: 347. Loss: 0.02408607490360737. Rolling Accuracy: 0.871\n",
      "Epoch: 348. Loss: 0.021209318190813065. Rolling Accuracy: 0.862\n",
      "Epoch: 349. Loss: 0.04770297184586525. Rolling Accuracy: 0.866\n",
      "Epoch: 350. Loss: 0.05551752820611. Rolling Accuracy: 0.858\n",
      "Epoch: 351. Loss: 0.03782755881547928. Rolling Accuracy: 0.871\n",
      "Epoch: 352. Loss: 0.03036552667617798. Rolling Accuracy: 0.862\n",
      "Epoch: 353. Loss: 0.028415052220225334. Rolling Accuracy: 0.871\n",
      "Epoch: 354. Loss: 0.034565623849630356. Rolling Accuracy: 0.88\n",
      "Epoch: 355. Loss: 0.02815435454249382. Rolling Accuracy: 0.88\n",
      "Epoch: 356. Loss: 0.036779072135686874. Rolling Accuracy: 0.892\n",
      "Saving model at epoch 356.\n",
      "Epoch: 357. Loss: 0.020833704620599747. Rolling Accuracy: 0.888\n",
      "Epoch: 358. Loss: 0.03340831398963928. Rolling Accuracy: 0.892\n",
      "Epoch: 359. Loss: 0.026100918650627136. Rolling Accuracy: 0.884\n",
      "Epoch: 360. Loss: 0.0252362210303545. Rolling Accuracy: 0.884\n",
      "Epoch: 361. Loss: 0.027331596240401268. Rolling Accuracy: 0.875\n",
      "Epoch: 362. Loss: 0.02902202308177948. Rolling Accuracy: 0.871\n",
      "Epoch: 363. Loss: 0.026116469874978065. Rolling Accuracy: 0.871\n",
      "Epoch: 364. Loss: 0.02639402076601982. Rolling Accuracy: 0.866\n",
      "Epoch: 365. Loss: 0.02842136099934578. Rolling Accuracy: 0.862\n",
      "Epoch: 366. Loss: 0.02314799651503563. Rolling Accuracy: 0.866\n",
      "Epoch: 367. Loss: 0.0227983295917511. Rolling Accuracy: 0.866\n",
      "Epoch: 368. Loss: 0.022120963782072067. Rolling Accuracy: 0.875\n",
      "Epoch: 369. Loss: 0.0240581426769495. Rolling Accuracy: 0.896\n",
      "Saving model at epoch 369.\n",
      "Epoch: 370. Loss: 0.02428370900452137. Rolling Accuracy: 0.896\n",
      "Epoch: 371. Loss: 0.024329865351319313. Rolling Accuracy: 0.901\n",
      "Saving model at epoch 371.\n",
      "Epoch: 372. Loss: 0.02259894646704197. Rolling Accuracy: 0.901\n",
      "Epoch: 373. Loss: 0.023460308089852333. Rolling Accuracy: 0.888\n",
      "Epoch: 374. Loss: 0.021535472944378853. Rolling Accuracy: 0.893\n",
      "Epoch: 375. Loss: 0.022771432995796204. Rolling Accuracy: 0.888\n",
      "Epoch: 376. Loss: 0.023843122646212578. Rolling Accuracy: 0.879\n",
      "Epoch: 377. Loss: 0.02121089957654476. Rolling Accuracy: 0.884\n",
      "Epoch: 378. Loss: 0.0253347959369421. Rolling Accuracy: 0.875\n",
      "Epoch: 379. Loss: 0.019425993785262108. Rolling Accuracy: 0.867\n",
      "Epoch: 380. Loss: 0.01834895834326744. Rolling Accuracy: 0.871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 381. Loss: 0.04581793397665024. Rolling Accuracy: 0.871\n",
      "Epoch: 382. Loss: 0.030584771186113358. Rolling Accuracy: 0.871\n",
      "Epoch: 383. Loss: 0.0230748001486063. Rolling Accuracy: 0.884\n",
      "Epoch: 384. Loss: 0.036182235926389694. Rolling Accuracy: 0.884\n",
      "Epoch: 385. Loss: 0.018608959391713142. Rolling Accuracy: 0.875\n",
      "Epoch: 386. Loss: 0.023121122270822525. Rolling Accuracy: 0.879\n",
      "Epoch: 387. Loss: 0.021892575547099113. Rolling Accuracy: 0.866\n",
      "Epoch: 388. Loss: 0.024748992174863815. Rolling Accuracy: 0.871\n",
      "Epoch: 389. Loss: 0.01962689496576786. Rolling Accuracy: 0.871\n",
      "Epoch: 390. Loss: 0.021180594339966774. Rolling Accuracy: 0.879\n",
      "Epoch: 391. Loss: 0.02124510332942009. Rolling Accuracy: 0.893\n",
      "Epoch: 392. Loss: 0.024761736392974854. Rolling Accuracy: 0.888\n",
      "Epoch: 393. Loss: 0.02269885316491127. Rolling Accuracy: 0.901\n",
      "Epoch: 394. Loss: 0.020775992423295975. Rolling Accuracy: 0.897\n",
      "Epoch: 395. Loss: 0.022947072982788086. Rolling Accuracy: 0.901\n",
      "Saving model at epoch 395.\n",
      "Epoch: 396. Loss: 0.020222563296556473. Rolling Accuracy: 0.906\n",
      "Saving model at epoch 396.\n",
      "Epoch: 397. Loss: 0.02337677776813507. Rolling Accuracy: 0.901\n",
      "Epoch: 398. Loss: 0.014297599904239178. Rolling Accuracy: 0.888\n",
      "Epoch: 399. Loss: 0.022913943976163864. Rolling Accuracy: 0.884\n",
      "Epoch: 400. Loss: 0.020902639254927635. Rolling Accuracy: 0.875\n",
      "Epoch: 401. Loss: 0.024959515780210495. Rolling Accuracy: 0.875\n",
      "Epoch: 402. Loss: 0.018489211797714233. Rolling Accuracy: 0.884\n",
      "Epoch: 403. Loss: 0.015353242866694927. Rolling Accuracy: 0.888\n",
      "Epoch: 404. Loss: 0.017710568383336067. Rolling Accuracy: 0.897\n",
      "Epoch: 405. Loss: 0.018082814291119576. Rolling Accuracy: 0.897\n",
      "Epoch: 406. Loss: 0.02353338897228241. Rolling Accuracy: 0.906\n",
      "Epoch: 407. Loss: 0.024560358375310898. Rolling Accuracy: 0.88\n",
      "Epoch: 408. Loss: 0.03032870590686798. Rolling Accuracy: 0.875\n",
      "Epoch: 409. Loss: 0.05313051864504814. Rolling Accuracy: 0.862\n",
      "Epoch: 410. Loss: 0.02231813408434391. Rolling Accuracy: 0.849\n",
      "Epoch: 411. Loss: 0.015971897169947624. Rolling Accuracy: 0.866\n",
      "Epoch: 412. Loss: 0.014676897786557674. Rolling Accuracy: 0.866\n",
      "Epoch: 413. Loss: 0.02236407808959484. Rolling Accuracy: 0.875\n",
      "Epoch: 414. Loss: 0.01912424899637699. Rolling Accuracy: 0.884\n",
      "Epoch: 415. Loss: 0.01718803495168686. Rolling Accuracy: 0.879\n",
      "Epoch: 416. Loss: 0.017189692705869675. Rolling Accuracy: 0.892\n",
      "Epoch: 417. Loss: 0.01944654993712902. Rolling Accuracy: 0.901\n",
      "Epoch: 418. Loss: 0.014987555332481861. Rolling Accuracy: 0.892\n",
      "Epoch: 419. Loss: 0.019087370485067368. Rolling Accuracy: 0.901\n",
      "Epoch: 420. Loss: 0.014057934284210205. Rolling Accuracy: 0.879\n",
      "Epoch: 421. Loss: 0.016507890075445175. Rolling Accuracy: 0.879\n",
      "Epoch: 422. Loss: 0.020954152569174767. Rolling Accuracy: 0.893\n",
      "Epoch: 423. Loss: 0.017097067087888718. Rolling Accuracy: 0.879\n",
      "Epoch: 424. Loss: 0.0151230962947011. Rolling Accuracy: 0.893\n",
      "Epoch: 425. Loss: 0.014776910655200481. Rolling Accuracy: 0.893\n",
      "Epoch: 426. Loss: 0.01886007748544216. Rolling Accuracy: 0.879\n",
      "Epoch: 427. Loss: 0.01908913068473339. Rolling Accuracy: 0.884\n",
      "Epoch: 428. Loss: 0.014312737621366978. Rolling Accuracy: 0.879\n",
      "Epoch: 429. Loss: 0.014281133189797401. Rolling Accuracy: 0.871\n",
      "Epoch: 430. Loss: 0.014258706010878086. Rolling Accuracy: 0.879\n",
      "Epoch: 431. Loss: 0.013940987177193165. Rolling Accuracy: 0.888\n",
      "Epoch: 432. Loss: 0.017272567376494408. Rolling Accuracy: 0.888\n",
      "Epoch: 433. Loss: 0.011834224686026573. Rolling Accuracy: 0.884\n",
      "Epoch: 434. Loss: 0.014670954085886478. Rolling Accuracy: 0.888\n",
      "Epoch: 435. Loss: 0.015984583646059036. Rolling Accuracy: 0.884\n",
      "Epoch: 436. Loss: 0.01684018038213253. Rolling Accuracy: 0.892\n",
      "Epoch: 437. Loss: 0.013693047687411308. Rolling Accuracy: 0.905\n",
      "Epoch: 438. Loss: 0.024174785241484642. Rolling Accuracy: 0.896\n",
      "Epoch: 439. Loss: 0.04832647368311882. Rolling Accuracy: 0.892\n",
      "Epoch: 440. Loss: 0.11901408433914185. Rolling Accuracy: 0.879\n",
      "Epoch: 441. Loss: 0.024816304445266724. Rolling Accuracy: 0.875\n",
      "Epoch: 442. Loss: 0.018984820693731308. Rolling Accuracy: 0.871\n",
      "Epoch: 443. Loss: 0.01736277900636196. Rolling Accuracy: 0.892\n",
      "Epoch: 444. Loss: 0.016823429614305496. Rolling Accuracy: 0.909\n",
      "Saving model at epoch 444.\n",
      "Epoch: 445. Loss: 0.013264975510537624. Rolling Accuracy: 0.905\n",
      "Epoch: 446. Loss: 0.013236665166914463. Rolling Accuracy: 0.914\n",
      "Saving model at epoch 446.\n",
      "Epoch: 447. Loss: 0.012918269261717796. Rolling Accuracy: 0.901\n",
      "Epoch: 448. Loss: 0.03044281341135502. Rolling Accuracy: 0.888\n",
      "Epoch: 449. Loss: 0.02065325155854225. Rolling Accuracy: 0.888\n",
      "Epoch: 450. Loss: 0.011213603429496288. Rolling Accuracy: 0.888\n",
      "Epoch: 451. Loss: 0.012121206149458885. Rolling Accuracy: 0.884\n",
      "Epoch: 452. Loss: 0.015117049217224121. Rolling Accuracy: 0.884\n",
      "Epoch: 453. Loss: 0.014660057611763477. Rolling Accuracy: 0.884\n",
      "Epoch: 454. Loss: 0.014441684819757938. Rolling Accuracy: 0.879\n",
      "Epoch: 455. Loss: 0.013735425658524036. Rolling Accuracy: 0.875\n",
      "Epoch: 456. Loss: 0.013656155206263065. Rolling Accuracy: 0.884\n",
      "Epoch: 457. Loss: 0.01089396420866251. Rolling Accuracy: 0.884\n",
      "Epoch: 458. Loss: 0.013410212472081184. Rolling Accuracy: 0.892\n",
      "Epoch: 459. Loss: 0.014596022665500641. Rolling Accuracy: 0.905\n",
      "Epoch: 460. Loss: 0.009920665994286537. Rolling Accuracy: 0.897\n",
      "Epoch: 461. Loss: 0.009280649945139885. Rolling Accuracy: 0.901\n",
      "Epoch: 462. Loss: 0.013920723460614681. Rolling Accuracy: 0.892\n",
      "Epoch: 463. Loss: 0.013420717790722847. Rolling Accuracy: 0.888\n",
      "Epoch: 464. Loss: 0.012119702994823456. Rolling Accuracy: 0.893\n",
      "Epoch: 465. Loss: 0.012370957061648369. Rolling Accuracy: 0.884\n",
      "Epoch: 466. Loss: 0.011646857485175133. Rolling Accuracy: 0.888\n",
      "Epoch: 467. Loss: 0.011128022335469723. Rolling Accuracy: 0.888\n",
      "Epoch: 468. Loss: 0.011075218208134174. Rolling Accuracy: 0.888\n",
      "Epoch: 469. Loss: 0.011019977740943432. Rolling Accuracy: 0.897\n",
      "Epoch: 470. Loss: 0.010567541234195232. Rolling Accuracy: 0.897\n",
      "Epoch: 471. Loss: 0.010436617769300938. Rolling Accuracy: 0.897\n",
      "Epoch: 472. Loss: 0.008741196244955063. Rolling Accuracy: 0.893\n",
      "Epoch: 473. Loss: 0.008921697735786438. Rolling Accuracy: 0.893\n",
      "Epoch: 474. Loss: 0.009229031391441822. Rolling Accuracy: 0.893\n",
      "Epoch: 475. Loss: 0.009112484753131866. Rolling Accuracy: 0.893\n",
      "Epoch: 476. Loss: 0.009331462904810905. Rolling Accuracy: 0.893\n",
      "Epoch: 477. Loss: 0.008454126305878162. Rolling Accuracy: 0.893\n",
      "Epoch: 478. Loss: 0.025611650198698044. Rolling Accuracy: 0.888\n",
      "Epoch: 479. Loss: 0.05033732205629349. Rolling Accuracy: 0.879\n",
      "Epoch: 480. Loss: 0.024869034066796303. Rolling Accuracy: 0.888\n",
      "Epoch: 481. Loss: 0.0199135672301054. Rolling Accuracy: 0.879\n",
      "Epoch: 482. Loss: 0.012379496358335018. Rolling Accuracy: 0.884\n",
      "Epoch: 483. Loss: 0.014634897001087666. Rolling Accuracy: 0.888\n",
      "Epoch: 484. Loss: 0.013449504040181637. Rolling Accuracy: 0.879\n",
      "Epoch: 485. Loss: 0.014358066953718662. Rolling Accuracy: 0.892\n",
      "Epoch: 486. Loss: 0.012725180014967918. Rolling Accuracy: 0.892\n",
      "Epoch: 487. Loss: 0.01067333109676838. Rolling Accuracy: 0.897\n",
      "Epoch: 488. Loss: 0.11591433733701706. Rolling Accuracy: 0.867\n",
      "Epoch: 489. Loss: 0.03685704618692398. Rolling Accuracy: 0.858\n",
      "Epoch: 490. Loss: 0.20460522174835205. Rolling Accuracy: 0.793\n",
      "Epoch: 491. Loss: 0.18391010165214539. Rolling Accuracy: 0.776\n",
      "Epoch: 492. Loss: 0.08546849340200424. Rolling Accuracy: 0.815\n",
      "Epoch: 493. Loss: 0.032876286655664444. Rolling Accuracy: 0.819\n",
      "Epoch: 494. Loss: 0.053345441818237305. Rolling Accuracy: 0.867\n",
      "Epoch: 495. Loss: 0.015054971911013126. Rolling Accuracy: 0.884\n",
      "Epoch: 496. Loss: 0.01376052014529705. Rolling Accuracy: 0.88\n",
      "Epoch: 497. Loss: 0.012501749210059643. Rolling Accuracy: 0.875\n",
      "Epoch: 498. Loss: 0.014771399088203907. Rolling Accuracy: 0.888\n",
      "Epoch: 499. Loss: 0.01146924402564764. Rolling Accuracy: 0.888\n",
      "Epoch: 500. Loss: 0.011123588308691978. Rolling Accuracy: 0.892\n",
      "Epoch: 501. Loss: 0.012353158555924892. Rolling Accuracy: 0.897\n",
      "Epoch: 502. Loss: 0.013465024530887604. Rolling Accuracy: 0.901\n",
      "Epoch: 503. Loss: 0.00891560222953558. Rolling Accuracy: 0.897\n",
      "Epoch: 504. Loss: 0.007609437685459852. Rolling Accuracy: 0.888\n",
      "Epoch: 505. Loss: 0.010365156456828117. Rolling Accuracy: 0.884\n",
      "Epoch: 506. Loss: 0.009921766817569733. Rolling Accuracy: 0.884\n",
      "Epoch: 507. Loss: 0.010346232913434505. Rolling Accuracy: 0.888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 508. Loss: 0.009857363067567348. Rolling Accuracy: 0.888\n",
      "Epoch: 509. Loss: 0.006505575031042099. Rolling Accuracy: 0.888\n",
      "Epoch: 510. Loss: 0.008207350037992. Rolling Accuracy: 0.888\n",
      "Epoch: 511. Loss: 0.009927055798470974. Rolling Accuracy: 0.884\n",
      "Epoch: 512. Loss: 0.008205131627619267. Rolling Accuracy: 0.892\n",
      "Epoch: 513. Loss: 0.006350950337946415. Rolling Accuracy: 0.892\n",
      "Epoch: 514. Loss: 0.007535705808550119. Rolling Accuracy: 0.892\n",
      "Epoch: 515. Loss: 0.010248412378132343. Rolling Accuracy: 0.892\n",
      "Epoch: 516. Loss: 0.006442480720579624. Rolling Accuracy: 0.884\n",
      "Epoch: 517. Loss: 0.006856301333755255. Rolling Accuracy: 0.888\n",
      "Epoch: 518. Loss: 0.00923363771289587. Rolling Accuracy: 0.892\n",
      "Epoch: 519. Loss: 0.00902984756976366. Rolling Accuracy: 0.892\n",
      "Epoch: 520. Loss: 0.00747970724478364. Rolling Accuracy: 0.892\n",
      "Epoch: 521. Loss: 0.006776456255465746. Rolling Accuracy: 0.897\n",
      "Epoch: 522. Loss: 0.006859208457171917. Rolling Accuracy: 0.892\n",
      "Epoch: 523. Loss: 0.006673215422779322. Rolling Accuracy: 0.892\n",
      "Epoch: 524. Loss: 0.006560732610523701. Rolling Accuracy: 0.892\n",
      "Epoch: 525. Loss: 0.006245408207178116. Rolling Accuracy: 0.892\n",
      "Epoch: 526. Loss: 0.006244115065783262. Rolling Accuracy: 0.888\n",
      "Epoch: 527. Loss: 0.006192511413246393. Rolling Accuracy: 0.892\n",
      "Epoch: 528. Loss: 0.005743362475186586. Rolling Accuracy: 0.901\n",
      "Epoch: 529. Loss: 0.010127286426723003. Rolling Accuracy: 0.892\n",
      "Epoch: 530. Loss: 0.01234333124011755. Rolling Accuracy: 0.892\n",
      "Epoch: 531. Loss: 0.009462890215218067. Rolling Accuracy: 0.892\n",
      "Epoch: 532. Loss: 0.0066477530635893345. Rolling Accuracy: 0.892\n",
      "Epoch: 533. Loss: 0.005766637157648802. Rolling Accuracy: 0.892\n",
      "Epoch: 534. Loss: 0.006790746934711933. Rolling Accuracy: 0.897\n",
      "Epoch: 535. Loss: 0.0070135327987372875. Rolling Accuracy: 0.897\n",
      "Epoch: 536. Loss: 0.006566779222339392. Rolling Accuracy: 0.893\n",
      "Epoch: 537. Loss: 0.006528866942971945. Rolling Accuracy: 0.897\n",
      "Epoch: 538. Loss: 0.006032027304172516. Rolling Accuracy: 0.897\n",
      "Epoch: 539. Loss: 0.005801468621939421. Rolling Accuracy: 0.897\n",
      "Epoch: 540. Loss: 0.005746294278651476. Rolling Accuracy: 0.897\n",
      "Epoch: 541. Loss: 0.005564883816987276. Rolling Accuracy: 0.897\n",
      "Epoch: 542. Loss: 0.005536933895200491. Rolling Accuracy: 0.897\n",
      "Epoch: 543. Loss: 0.005253676790744066. Rolling Accuracy: 0.897\n",
      "Epoch: 544. Loss: 0.005186147056519985. Rolling Accuracy: 0.893\n",
      "Epoch: 545. Loss: 0.00525880279019475. Rolling Accuracy: 0.888\n",
      "Epoch: 546. Loss: 0.005601143930107355. Rolling Accuracy: 0.883\n",
      "Epoch: 547. Loss: 0.005080830305814743. Rolling Accuracy: 0.879\n",
      "Epoch: 548. Loss: 0.00496759032830596. Rolling Accuracy: 0.888\n",
      "Epoch: 549. Loss: 0.004818658344447613. Rolling Accuracy: 0.892\n",
      "Epoch: 550. Loss: 0.00529984338209033. Rolling Accuracy: 0.897\n",
      "Epoch: 551. Loss: 0.0038029744755476713. Rolling Accuracy: 0.893\n",
      "Epoch: 552. Loss: 0.005957481451332569. Rolling Accuracy: 0.888\n",
      "Epoch: 553. Loss: 0.007472245953977108. Rolling Accuracy: 0.884\n",
      "Epoch: 554. Loss: 0.005027272738516331. Rolling Accuracy: 0.884\n",
      "Epoch: 555. Loss: 0.005453248508274555. Rolling Accuracy: 0.893\n",
      "Epoch: 556. Loss: 0.004984068684279919. Rolling Accuracy: 0.893\n",
      "Epoch: 557. Loss: 0.004011243116110563. Rolling Accuracy: 0.893\n",
      "Epoch: 558. Loss: 0.004984075669199228. Rolling Accuracy: 0.893\n",
      "Epoch: 559. Loss: 0.004741888027638197. Rolling Accuracy: 0.893\n",
      "Epoch: 560. Loss: 0.004031929187476635. Rolling Accuracy: 0.893\n",
      "Epoch: 561. Loss: 0.007835046388208866. Rolling Accuracy: 0.893\n",
      "Epoch: 562. Loss: 0.3215606212615967. Rolling Accuracy: 0.879\n",
      "Epoch: 563. Loss: 0.14181964099407196. Rolling Accuracy: 0.862\n",
      "Epoch: 564. Loss: 0.01985747180879116. Rolling Accuracy: 0.866\n",
      "Epoch: 565. Loss: 0.031247006729245186. Rolling Accuracy: 0.858\n",
      "Epoch: 566. Loss: 0.056933995336294174. Rolling Accuracy: 0.858\n",
      "Epoch: 567. Loss: 0.01801176182925701. Rolling Accuracy: 0.88\n",
      "Epoch: 568. Loss: 0.010246865451335907. Rolling Accuracy: 0.866\n",
      "Epoch: 569. Loss: 0.008476438000798225. Rolling Accuracy: 0.875\n",
      "Epoch: 570. Loss: 0.008884360082447529. Rolling Accuracy: 0.892\n",
      "Epoch: 571. Loss: 0.009920639917254448. Rolling Accuracy: 0.888\n",
      "Epoch: 572. Loss: 0.009921128861606121. Rolling Accuracy: 0.897\n",
      "Epoch: 573. Loss: 0.008449099957942963. Rolling Accuracy: 0.897\n",
      "Epoch: 574. Loss: 0.006866419222205877. Rolling Accuracy: 0.888\n",
      "Epoch: 575. Loss: 0.0070025804452598095. Rolling Accuracy: 0.892\n",
      "Epoch: 576. Loss: 0.006460264325141907. Rolling Accuracy: 0.888\n",
      "Epoch: 577. Loss: 0.0048981779254972935. Rolling Accuracy: 0.892\n",
      "Epoch: 578. Loss: 0.00800082553178072. Rolling Accuracy: 0.897\n",
      "Epoch: 579. Loss: 0.008084860630333424. Rolling Accuracy: 0.893\n",
      "Epoch: 580. Loss: 0.005875811446458101. Rolling Accuracy: 0.893\n",
      "Epoch: 581. Loss: 0.005131647922098637. Rolling Accuracy: 0.901\n",
      "Epoch: 582. Loss: 0.004628962371498346. Rolling Accuracy: 0.909\n",
      "Epoch: 583. Loss: 0.005819203797727823. Rolling Accuracy: 0.909\n",
      "Epoch: 584. Loss: 0.004202710464596748. Rolling Accuracy: 0.909\n",
      "Epoch: 585. Loss: 0.007917826995253563. Rolling Accuracy: 0.905\n",
      "Epoch: 586. Loss: 0.053281668573617935. Rolling Accuracy: 0.875\n",
      "Epoch: 587. Loss: 0.008455591276288033. Rolling Accuracy: 0.879\n",
      "Epoch: 588. Loss: 0.013022789731621742. Rolling Accuracy: 0.888\n",
      "Epoch: 589. Loss: 0.008988228626549244. Rolling Accuracy: 0.879\n",
      "Epoch: 590. Loss: 0.011492564342916012. Rolling Accuracy: 0.905\n",
      "Epoch: 591. Loss: 0.007341480348259211. Rolling Accuracy: 0.905\n",
      "Epoch: 592. Loss: 0.004833615384995937. Rolling Accuracy: 0.892\n",
      "Epoch: 593. Loss: 0.00572544988244772. Rolling Accuracy: 0.897\n",
      "Epoch: 594. Loss: 0.005709365475922823. Rolling Accuracy: 0.892\n",
      "Epoch: 595. Loss: 0.004251204896718264. Rolling Accuracy: 0.892\n",
      "Epoch: 596. Loss: 0.00484097795560956. Rolling Accuracy: 0.901\n",
      "Epoch: 597. Loss: 0.005387910176068544. Rolling Accuracy: 0.901\n",
      "Epoch: 598. Loss: 0.005069141276180744. Rolling Accuracy: 0.901\n",
      "Epoch: 599. Loss: 0.004308260511606932. Rolling Accuracy: 0.901\n",
      "Epoch: 600. Loss: 0.0035743582993745804. Rolling Accuracy: 0.888\n",
      "Epoch: 601. Loss: 0.009443996474146843. Rolling Accuracy: 0.888\n",
      "Epoch: 602. Loss: 0.007804481778293848. Rolling Accuracy: 0.88\n",
      "Epoch: 603. Loss: 0.06240390986204147. Rolling Accuracy: 0.871\n",
      "Epoch: 604. Loss: 0.11308930814266205. Rolling Accuracy: 0.888\n",
      "Epoch: 605. Loss: 0.009106988087296486. Rolling Accuracy: 0.888\n",
      "Epoch: 606. Loss: 0.011619317345321178. Rolling Accuracy: 0.888\n",
      "Epoch: 607. Loss: 0.016346853226423264. Rolling Accuracy: 0.893\n",
      "Epoch: 608. Loss: 0.013674885965883732. Rolling Accuracy: 0.893\n",
      "Epoch: 609. Loss: 0.008448847569525242. Rolling Accuracy: 0.893\n",
      "Epoch: 610. Loss: 0.015721891075372696. Rolling Accuracy: 0.897\n",
      "Epoch: 611. Loss: 0.02215428091585636. Rolling Accuracy: 0.892\n",
      "Epoch: 612. Loss: 0.014887715689837933. Rolling Accuracy: 0.888\n",
      "Epoch: 613. Loss: 0.01029157917946577. Rolling Accuracy: 0.892\n",
      "Epoch: 614. Loss: 0.009317399933934212. Rolling Accuracy: 0.901\n",
      "Epoch: 615. Loss: 0.009031476452946663. Rolling Accuracy: 0.905\n",
      "Epoch: 616. Loss: 0.007795319426804781. Rolling Accuracy: 0.905\n",
      "Epoch: 617. Loss: 0.007956149987876415. Rolling Accuracy: 0.901\n",
      "Epoch: 618. Loss: 0.008138624019920826. Rolling Accuracy: 0.897\n",
      "Epoch: 619. Loss: 0.006093121133744717. Rolling Accuracy: 0.897\n",
      "Epoch: 620. Loss: 0.01879584789276123. Rolling Accuracy: 0.893\n",
      "Epoch: 621. Loss: 0.06933999806642532. Rolling Accuracy: 0.879\n",
      "Epoch: 622. Loss: 0.03830377385020256. Rolling Accuracy: 0.871\n",
      "Epoch: 623. Loss: 0.0034799601417034864. Rolling Accuracy: 0.875\n",
      "Epoch: 624. Loss: 0.005290229804813862. Rolling Accuracy: 0.875\n",
      "Epoch: 625. Loss: 0.004104891791939735. Rolling Accuracy: 0.879\n",
      "Epoch: 626. Loss: 0.003216568846255541. Rolling Accuracy: 0.888\n",
      "Epoch: 627. Loss: 0.0051878588274121284. Rolling Accuracy: 0.879\n",
      "Epoch: 628. Loss: 0.004534613806754351. Rolling Accuracy: 0.884\n",
      "Epoch: 629. Loss: 0.0036675615701824427. Rolling Accuracy: 0.888\n",
      "Epoch: 630. Loss: 0.0031482786871492863. Rolling Accuracy: 0.888\n",
      "Epoch: 631. Loss: 0.0026426909025758505. Rolling Accuracy: 0.893\n",
      "Epoch: 632. Loss: 0.0025316576939076185. Rolling Accuracy: 0.893\n",
      "Epoch: 633. Loss: 0.00287624285556376. Rolling Accuracy: 0.897\n",
      "Epoch: 634. Loss: 0.0031009861268103123. Rolling Accuracy: 0.897\n",
      "Epoch: 635. Loss: 0.0030004314612597227. Rolling Accuracy: 0.897\n",
      "Epoch: 636. Loss: 0.0028108241967856884. Rolling Accuracy: 0.897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 637. Loss: 0.002760716248303652. Rolling Accuracy: 0.897\n",
      "Epoch: 638. Loss: 0.0026690897066146135. Rolling Accuracy: 0.897\n",
      "Epoch: 639. Loss: 0.00256914971396327. Rolling Accuracy: 0.897\n",
      "Epoch: 640. Loss: 0.0024015307426452637. Rolling Accuracy: 0.888\n",
      "Epoch: 641. Loss: 0.0035642858128994703. Rolling Accuracy: 0.884\n",
      "Epoch: 642. Loss: 0.010062525048851967. Rolling Accuracy: 0.892\n",
      "Epoch: 643. Loss: 0.008472559042274952. Rolling Accuracy: 0.888\n",
      "Epoch: 644. Loss: 0.007695671170949936. Rolling Accuracy: 0.896\n",
      "Epoch: 645. Loss: 0.008950620889663696. Rolling Accuracy: 0.91\n",
      "Epoch: 646. Loss: 0.008067918941378593. Rolling Accuracy: 0.905\n",
      "Epoch: 647. Loss: 0.006543498486280441. Rolling Accuracy: 0.914\n",
      "Saving model at epoch 647.\n",
      "Epoch: 648. Loss: 0.00457717152312398. Rolling Accuracy: 0.918\n",
      "Saving model at epoch 648.\n",
      "Epoch: 649. Loss: 0.004047691822052002. Rolling Accuracy: 0.914\n",
      "Epoch: 650. Loss: 0.003933355212211609. Rolling Accuracy: 0.91\n",
      "Epoch: 651. Loss: 0.003404794028028846. Rolling Accuracy: 0.91\n",
      "Epoch: 652. Loss: 0.0031809082720428705. Rolling Accuracy: 0.91\n",
      "Epoch: 653. Loss: 0.003197988262400031. Rolling Accuracy: 0.906\n",
      "Epoch: 654. Loss: 0.0030365849379450083. Rolling Accuracy: 0.906\n",
      "Epoch: 655. Loss: 0.003910598810762167. Rolling Accuracy: 0.901\n",
      "Epoch: 656. Loss: 0.004643914755433798. Rolling Accuracy: 0.897\n",
      "Epoch: 657. Loss: 0.0038942282553762197. Rolling Accuracy: 0.897\n",
      "Epoch: 658. Loss: 0.002926500281319022. Rolling Accuracy: 0.901\n",
      "Epoch: 659. Loss: 0.0029121136758476496. Rolling Accuracy: 0.901\n",
      "Epoch: 660. Loss: 0.0027880785055458546. Rolling Accuracy: 0.901\n",
      "Epoch: 661. Loss: 0.002584020374342799. Rolling Accuracy: 0.901\n",
      "Epoch: 662. Loss: 0.0025131215807050467. Rolling Accuracy: 0.897\n",
      "Epoch: 663. Loss: 0.002503248630091548. Rolling Accuracy: 0.897\n",
      "Epoch: 664. Loss: 0.002479814225807786. Rolling Accuracy: 0.897\n",
      "Epoch: 665. Loss: 0.0024559299927204847. Rolling Accuracy: 0.897\n",
      "Epoch: 666. Loss: 0.0023125114385038614. Rolling Accuracy: 0.901\n",
      "Epoch: 667. Loss: 0.0024965968914330006. Rolling Accuracy: 0.901\n",
      "Epoch: 668. Loss: 0.0024251420982182026. Rolling Accuracy: 0.901\n",
      "Epoch: 669. Loss: 0.002116214018315077. Rolling Accuracy: 0.901\n",
      "Epoch: 670. Loss: 0.0020496374927461147. Rolling Accuracy: 0.897\n",
      "Epoch: 671. Loss: 0.01640428975224495. Rolling Accuracy: 0.892\n",
      "Epoch: 672. Loss: 0.03089030459523201. Rolling Accuracy: 0.888\n",
      "Epoch: 673. Loss: 0.012498917058110237. Rolling Accuracy: 0.892\n",
      "Epoch: 674. Loss: 0.0048824879340827465. Rolling Accuracy: 0.892\n",
      "Epoch: 675. Loss: 0.004889013711363077. Rolling Accuracy: 0.897\n",
      "Epoch: 676. Loss: 0.004564950708299875. Rolling Accuracy: 0.901\n",
      "Epoch: 677. Loss: 0.004635713063180447. Rolling Accuracy: 0.897\n",
      "Epoch: 678. Loss: 0.004443887155503035. Rolling Accuracy: 0.897\n",
      "Epoch: 679. Loss: 0.003160076215863228. Rolling Accuracy: 0.901\n",
      "Epoch: 680. Loss: 0.0020718041341751814. Rolling Accuracy: 0.901\n",
      "Epoch: 681. Loss: 0.004025564529001713. Rolling Accuracy: 0.906\n",
      "Epoch: 682. Loss: 0.0034731533378362656. Rolling Accuracy: 0.906\n",
      "Epoch: 683. Loss: 0.00275080231949687. Rolling Accuracy: 0.901\n",
      "Epoch: 684. Loss: 0.0036886322777718306. Rolling Accuracy: 0.901\n",
      "Epoch: 685. Loss: 0.004596348386257887. Rolling Accuracy: 0.897\n",
      "Epoch: 686. Loss: 0.0037422936875373125. Rolling Accuracy: 0.897\n",
      "Epoch: 687. Loss: 0.0026273259427398443. Rolling Accuracy: 0.897\n",
      "Epoch: 688. Loss: 0.0024838303215801716. Rolling Accuracy: 0.901\n",
      "Epoch: 689. Loss: 0.002702996600419283. Rolling Accuracy: 0.905\n",
      "Epoch: 690. Loss: 0.0022643485572189093. Rolling Accuracy: 0.905\n",
      "Epoch: 691. Loss: 0.0020336895249783993. Rolling Accuracy: 0.905\n",
      "Epoch: 692. Loss: 0.002550685778260231. Rolling Accuracy: 0.901\n",
      "Epoch: 693. Loss: 0.0028120537754148245. Rolling Accuracy: 0.897\n",
      "Epoch: 694. Loss: 0.0026062168180942535. Rolling Accuracy: 0.897\n",
      "Epoch: 695. Loss: 0.002256732666864991. Rolling Accuracy: 0.897\n",
      "Epoch: 696. Loss: 0.0023264880292117596. Rolling Accuracy: 0.897\n",
      "Epoch: 697. Loss: 0.0024235339369624853. Rolling Accuracy: 0.897\n",
      "Epoch: 698. Loss: 0.002285416005179286. Rolling Accuracy: 0.897\n",
      "Epoch: 699. Loss: 0.0021861314307898283. Rolling Accuracy: 0.897\n",
      "Epoch: 700. Loss: 0.0020711482502520084. Rolling Accuracy: 0.897\n",
      "Epoch: 701. Loss: 0.002070255810394883. Rolling Accuracy: 0.897\n",
      "Epoch: 702. Loss: 0.0019893625285476446. Rolling Accuracy: 0.897\n",
      "Epoch: 703. Loss: 0.0018684073584154248. Rolling Accuracy: 0.897\n",
      "Epoch: 704. Loss: 0.00182612135540694. Rolling Accuracy: 0.901\n",
      "Epoch: 705. Loss: 0.003007464110851288. Rolling Accuracy: 0.897\n",
      "Epoch: 706. Loss: 0.0020245343912392855. Rolling Accuracy: 0.892\n",
      "Epoch: 707. Loss: 0.002131957793608308. Rolling Accuracy: 0.888\n",
      "Epoch: 708. Loss: 0.0024365228600800037. Rolling Accuracy: 0.884\n",
      "Epoch: 709. Loss: 0.0020509306341409683. Rolling Accuracy: 0.888\n",
      "Epoch: 710. Loss: 0.0014819996431469917. Rolling Accuracy: 0.892\n",
      "Epoch: 711. Loss: 0.0033149379305541515. Rolling Accuracy: 0.888\n",
      "Epoch: 712. Loss: 0.2520642578601837. Rolling Accuracy: 0.858\n",
      "Epoch: 713. Loss: 0.023981934413313866. Rolling Accuracy: 0.862\n",
      "Epoch: 714. Loss: 0.07766561955213547. Rolling Accuracy: 0.845\n",
      "Epoch: 715. Loss: 0.023628856986761093. Rolling Accuracy: 0.862\n",
      "Epoch: 716. Loss: 0.03324093669652939. Rolling Accuracy: 0.897\n",
      "Epoch: 717. Loss: 0.005737426690757275. Rolling Accuracy: 0.897\n",
      "Epoch: 718. Loss: 0.004645143635571003. Rolling Accuracy: 0.909\n",
      "Epoch: 719. Loss: 0.009364276193082333. Rolling Accuracy: 0.901\n",
      "Epoch: 720. Loss: 0.008151435293257236. Rolling Accuracy: 0.892\n",
      "Epoch: 721. Loss: 0.004448215942829847. Rolling Accuracy: 0.888\n",
      "Epoch: 722. Loss: 0.0037322789430618286. Rolling Accuracy: 0.893\n",
      "Epoch: 723. Loss: 0.004454509820789099. Rolling Accuracy: 0.893\n",
      "Epoch: 724. Loss: 0.0027357679791748524. Rolling Accuracy: 0.897\n",
      "Epoch: 725. Loss: 0.03584497421979904. Rolling Accuracy: 0.888\n",
      "Epoch: 726. Loss: 0.03944803401827812. Rolling Accuracy: 0.879\n",
      "Epoch: 727. Loss: 0.011993267573416233. Rolling Accuracy: 0.879\n",
      "Epoch: 728. Loss: 0.00451140571385622. Rolling Accuracy: 0.888\n",
      "Epoch: 729. Loss: 0.00388592341914773. Rolling Accuracy: 0.905\n",
      "Epoch: 730. Loss: 0.005718819797039032. Rolling Accuracy: 0.914\n",
      "Epoch: 731. Loss: 0.004135668743401766. Rolling Accuracy: 0.914\n",
      "Epoch: 732. Loss: 0.0033203104976564646. Rolling Accuracy: 0.905\n",
      "Epoch: 733. Loss: 0.002894018078222871. Rolling Accuracy: 0.897\n",
      "Epoch: 734. Loss: 0.002613580320030451. Rolling Accuracy: 0.897\n",
      "Epoch: 735. Loss: 0.0024597567971795797. Rolling Accuracy: 0.901\n",
      "Epoch: 736. Loss: 0.0027472940273582935. Rolling Accuracy: 0.906\n",
      "Epoch: 737. Loss: 0.0052934810519218445. Rolling Accuracy: 0.906\n",
      "Epoch: 738. Loss: 0.005203829612582922. Rolling Accuracy: 0.906\n",
      "Epoch: 739. Loss: 0.004031350836157799. Rolling Accuracy: 0.901\n",
      "Epoch: 740. Loss: 0.0031089356634765863. Rolling Accuracy: 0.901\n",
      "Epoch: 741. Loss: 0.0022526325192302465. Rolling Accuracy: 0.91\n",
      "Epoch: 742. Loss: 0.002252801088616252. Rolling Accuracy: 0.918\n",
      "Epoch: 743. Loss: 0.0021502117160707712. Rolling Accuracy: 0.927\n",
      "Saving model at epoch 743.\n",
      "Epoch: 744. Loss: 0.0018198082689195871. Rolling Accuracy: 0.927\n",
      "Epoch: 745. Loss: 0.0023665407206863165. Rolling Accuracy: 0.918\n",
      "Epoch: 746. Loss: 0.003120881039649248. Rolling Accuracy: 0.91\n",
      "Epoch: 747. Loss: 0.0027752702590078115. Rolling Accuracy: 0.901\n",
      "Epoch: 748. Loss: 0.002067875349894166. Rolling Accuracy: 0.901\n",
      "Epoch: 749. Loss: 0.0015439370181411505. Rolling Accuracy: 0.905\n",
      "Epoch: 750. Loss: 0.0016525231767445803. Rolling Accuracy: 0.91\n",
      "Epoch: 751. Loss: 0.001609575585462153. Rolling Accuracy: 0.914\n",
      "Epoch: 752. Loss: 0.0028659249655902386. Rolling Accuracy: 0.91\n",
      "Epoch: 753. Loss: 0.005322516430169344. Rolling Accuracy: 0.91\n",
      "Epoch: 754. Loss: 0.005028496030718088. Rolling Accuracy: 0.91\n",
      "Epoch: 755. Loss: 0.003409392898902297. Rolling Accuracy: 0.91\n",
      "Epoch: 756. Loss: 0.0022088005207479. Rolling Accuracy: 0.914\n",
      "Epoch: 757. Loss: 0.0016841936158016324. Rolling Accuracy: 0.914\n",
      "Epoch: 758. Loss: 0.0013988163555040956. Rolling Accuracy: 0.914\n",
      "Epoch: 759. Loss: 0.0013273110380396247. Rolling Accuracy: 0.914\n",
      "Epoch: 760. Loss: 0.001263785525225103. Rolling Accuracy: 0.914\n",
      "Epoch: 761. Loss: 0.001281338045373559. Rolling Accuracy: 0.914\n",
      "Epoch: 762. Loss: 0.0018380841938778758. Rolling Accuracy: 0.914\n",
      "Epoch: 763. Loss: 0.0017752995481714606. Rolling Accuracy: 0.914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 764. Loss: 0.0013345389161258936. Rolling Accuracy: 0.914\n",
      "Epoch: 765. Loss: 0.0012117968872189522. Rolling Accuracy: 0.918\n",
      "Epoch: 766. Loss: 0.000891120929736644. Rolling Accuracy: 0.918\n",
      "Epoch: 767. Loss: 0.001162125961855054. Rolling Accuracy: 0.918\n",
      "Epoch: 768. Loss: 0.0015862485161051154. Rolling Accuracy: 0.914\n",
      "Epoch: 769. Loss: 0.0016406329814344645. Rolling Accuracy: 0.906\n",
      "Epoch: 770. Loss: 0.0015745424898341298. Rolling Accuracy: 0.91\n",
      "Epoch: 771. Loss: 0.0012118435697630048. Rolling Accuracy: 0.91\n",
      "Epoch: 772. Loss: 0.0008953322540037334. Rolling Accuracy: 0.914\n",
      "Epoch: 773. Loss: 0.0018783779814839363. Rolling Accuracy: 0.923\n",
      "Epoch: 774. Loss: 0.0018575765425339341. Rolling Accuracy: 0.918\n",
      "Epoch: 775. Loss: 0.001374130486510694. Rolling Accuracy: 0.914\n",
      "Epoch: 776. Loss: 0.0014806464314460754. Rolling Accuracy: 0.91\n",
      "Epoch: 777. Loss: 0.00185579271055758. Rolling Accuracy: 0.901\n",
      "Epoch: 778. Loss: 0.001892650849185884. Rolling Accuracy: 0.897\n",
      "Epoch: 779. Loss: 0.0017266292124986649. Rolling Accuracy: 0.897\n",
      "Epoch: 780. Loss: 0.001497686025686562. Rolling Accuracy: 0.897\n",
      "Epoch: 781. Loss: 0.0014324253425002098. Rolling Accuracy: 0.897\n",
      "Epoch: 782. Loss: 0.0006107891676947474. Rolling Accuracy: 0.888\n",
      "Epoch: 783. Loss: 0.0014962402638047934. Rolling Accuracy: 0.893\n",
      "Epoch: 784. Loss: 0.004617705941200256. Rolling Accuracy: 0.893\n",
      "Epoch: 785. Loss: 0.002050757873803377. Rolling Accuracy: 0.901\n",
      "Epoch: 786. Loss: 0.0020879083313047886. Rolling Accuracy: 0.914\n",
      "Epoch: 787. Loss: 0.0028945861849933863. Rolling Accuracy: 0.91\n",
      "Epoch: 788. Loss: 0.0023103661369532347. Rolling Accuracy: 0.905\n",
      "Epoch: 789. Loss: 0.003256207797676325. Rolling Accuracy: 0.897\n",
      "Epoch: 790. Loss: 0.004298611544072628. Rolling Accuracy: 0.888\n",
      "Epoch: 791. Loss: 0.0020910496823489666. Rolling Accuracy: 0.888\n",
      "Epoch: 792. Loss: 0.0010829285020008683. Rolling Accuracy: 0.893\n",
      "Epoch: 793. Loss: 0.0009212135337293148. Rolling Accuracy: 0.893\n",
      "Epoch: 794. Loss: 0.0009968056110665202. Rolling Accuracy: 0.897\n",
      "Epoch: 795. Loss: 0.000942064099945128. Rolling Accuracy: 0.897\n",
      "Epoch: 796. Loss: 0.0007856884622015059. Rolling Accuracy: 0.901\n",
      "Epoch: 797. Loss: 0.001358254230581224. Rolling Accuracy: 0.901\n",
      "Epoch: 798. Loss: 0.0014183224411681294. Rolling Accuracy: 0.897\n",
      "Epoch: 799. Loss: 0.000782078190241009. Rolling Accuracy: 0.897\n",
      "Epoch: 800. Loss: 0.002447516890242696. Rolling Accuracy: 0.893\n",
      "Epoch: 801. Loss: 0.004069671966135502. Rolling Accuracy: 0.888\n",
      "Epoch: 802. Loss: 0.003492446383461356. Rolling Accuracy: 0.901\n",
      "Epoch: 803. Loss: 0.0060012126341462135. Rolling Accuracy: 0.901\n",
      "Epoch: 804. Loss: 0.013518228195607662. Rolling Accuracy: 0.901\n",
      "Epoch: 805. Loss: 0.12261968851089478. Rolling Accuracy: 0.879\n",
      "Epoch: 806. Loss: 0.1582825481891632. Rolling Accuracy: 0.854\n",
      "Epoch: 807. Loss: 0.04998957738280296. Rolling Accuracy: 0.841\n",
      "Epoch: 808. Loss: 0.07471661269664764. Rolling Accuracy: 0.823\n",
      "Epoch: 809. Loss: 0.04992023855447769. Rolling Accuracy: 0.837\n",
      "Epoch: 810. Loss: 0.0494873933494091. Rolling Accuracy: 0.849\n",
      "Epoch: 811. Loss: 0.049539972096681595. Rolling Accuracy: 0.867\n",
      "Epoch: 812. Loss: 0.0494682751595974. Rolling Accuracy: 0.884\n",
      "Epoch: 813. Loss: 0.0494542196393013. Rolling Accuracy: 0.897\n",
      "Epoch: 814. Loss: 0.049471817910671234. Rolling Accuracy: 0.901\n",
      "Epoch: 815. Loss: 0.04944675788283348. Rolling Accuracy: 0.897\n",
      "Epoch: 816. Loss: 0.04952426627278328. Rolling Accuracy: 0.884\n",
      "Epoch: 817. Loss: 0.047179605811834335. Rolling Accuracy: 0.884\n",
      "Epoch: 818. Loss: 0.049596596509218216. Rolling Accuracy: 0.884\n",
      "Epoch: 819. Loss: 0.044357508420944214. Rolling Accuracy: 0.888\n",
      "Epoch: 820. Loss: 0.01221434772014618. Rolling Accuracy: 0.901\n",
      "Epoch: 821. Loss: 0.022034581750631332. Rolling Accuracy: 0.91\n",
      "Epoch: 822. Loss: 0.023293396458029747. Rolling Accuracy: 0.918\n",
      "Epoch: 823. Loss: 0.0065494924783706665. Rolling Accuracy: 0.923\n",
      "Epoch: 824. Loss: 0.0025894264690577984. Rolling Accuracy: 0.927\n",
      "Epoch: 825. Loss: 0.0026577624958008528. Rolling Accuracy: 0.923\n",
      "Epoch: 826. Loss: 0.0036869661416858435. Rolling Accuracy: 0.914\n",
      "Epoch: 827. Loss: 0.0032785949297249317. Rolling Accuracy: 0.91\n",
      "Epoch: 828. Loss: 0.0019247226882725954. Rolling Accuracy: 0.906\n",
      "Epoch: 829. Loss: 0.002553362865000963. Rolling Accuracy: 0.906\n",
      "Epoch: 830. Loss: 0.002752160420641303. Rolling Accuracy: 0.91\n",
      "Epoch: 831. Loss: 0.002284677466377616. Rolling Accuracy: 0.905\n",
      "Epoch: 832. Loss: 0.006317674648016691. Rolling Accuracy: 0.905\n",
      "Epoch: 833. Loss: 0.007453934755176306. Rolling Accuracy: 0.862\n",
      "Epoch: 834. Loss: 0.016604404896497726. Rolling Accuracy: 0.862\n",
      "Epoch: 835. Loss: 0.0069481912069022655. Rolling Accuracy: 0.858\n",
      "Epoch: 836. Loss: 0.2172282189130783. Rolling Accuracy: 0.832\n",
      "Epoch: 837. Loss: 0.009192680940032005. Rolling Accuracy: 0.879\n",
      "Epoch: 838. Loss: 0.03289647772908211. Rolling Accuracy: 0.884\n",
      "Epoch: 839. Loss: 0.00716035719960928. Rolling Accuracy: 0.892\n",
      "Epoch: 840. Loss: 0.0042171659879386425. Rolling Accuracy: 0.923\n",
      "Epoch: 841. Loss: 0.004999374505132437. Rolling Accuracy: 0.923\n",
      "Epoch: 842. Loss: 0.003920731134712696. Rolling Accuracy: 0.923\n",
      "Epoch: 843. Loss: 0.002541618188843131. Rolling Accuracy: 0.931\n",
      "Saving model at epoch 843.\n",
      "Epoch: 844. Loss: 0.0018258672207593918. Rolling Accuracy: 0.931\n",
      "Epoch: 845. Loss: 0.0016285766614601016. Rolling Accuracy: 0.927\n",
      "Epoch: 846. Loss: 0.0018756341887637973. Rolling Accuracy: 0.922\n",
      "Epoch: 847. Loss: 0.001865721307694912. Rolling Accuracy: 0.914\n",
      "Epoch: 848. Loss: 0.0017874296754598618. Rolling Accuracy: 0.91\n",
      "Epoch: 849. Loss: 0.001581372693181038. Rolling Accuracy: 0.906\n",
      "Epoch: 850. Loss: 0.0014287446392700076. Rolling Accuracy: 0.906\n",
      "Epoch: 851. Loss: 0.0012848768383264542. Rolling Accuracy: 0.906\n",
      "Epoch: 852. Loss: 0.0013011172413825989. Rolling Accuracy: 0.906\n",
      "Epoch: 853. Loss: 0.0012537402799353004. Rolling Accuracy: 0.906\n",
      "Epoch: 854. Loss: 0.0011514202924445271. Rolling Accuracy: 0.901\n",
      "Epoch: 855. Loss: 0.0010521330405026674. Rolling Accuracy: 0.901\n",
      "Epoch: 856. Loss: 0.001016168505884707. Rolling Accuracy: 0.906\n",
      "Epoch: 857. Loss: 0.0010049870470538735. Rolling Accuracy: 0.91\n",
      "Epoch: 858. Loss: 0.0009671234292909503. Rolling Accuracy: 0.914\n",
      "Epoch: 859. Loss: 0.0009911974193528295. Rolling Accuracy: 0.91\n",
      "Epoch: 860. Loss: 0.0009647594997659326. Rolling Accuracy: 0.905\n",
      "Epoch: 861. Loss: 0.0009140897891484201. Rolling Accuracy: 0.901\n",
      "Epoch: 862. Loss: 0.0020689109805971384. Rolling Accuracy: 0.901\n",
      "Epoch: 863. Loss: 0.0028050635010004044. Rolling Accuracy: 0.906\n",
      "Epoch: 864. Loss: 0.0021285247057676315. Rolling Accuracy: 0.91\n",
      "Epoch: 865. Loss: 0.001544133061543107. Rolling Accuracy: 0.914\n",
      "Epoch: 866. Loss: 0.00197165971621871. Rolling Accuracy: 0.914\n",
      "Epoch: 867. Loss: 0.0026536975055933. Rolling Accuracy: 0.91\n",
      "Epoch: 868. Loss: 0.004045161418616772. Rolling Accuracy: 0.905\n",
      "Epoch: 869. Loss: 0.002745050936937332. Rolling Accuracy: 0.901\n",
      "Epoch: 870. Loss: 0.0010863632196560502. Rolling Accuracy: 0.897\n",
      "Epoch: 871. Loss: 0.0006299521191976964. Rolling Accuracy: 0.897\n",
      "Epoch: 872. Loss: 0.0007041810313239694. Rolling Accuracy: 0.897\n",
      "Epoch: 873. Loss: 0.001370410667732358. Rolling Accuracy: 0.897\n",
      "Epoch: 874. Loss: 0.0031613041646778584. Rolling Accuracy: 0.897\n",
      "Epoch: 875. Loss: 0.0027646857779473066. Rolling Accuracy: 0.897\n",
      "Epoch: 876. Loss: 0.0015919508878141642. Rolling Accuracy: 0.897\n",
      "Epoch: 877. Loss: 0.0009381479467265308. Rolling Accuracy: 0.901\n",
      "Epoch: 878. Loss: 0.0006029580836184323. Rolling Accuracy: 0.905\n",
      "Epoch: 879. Loss: 0.0005526415770873427. Rolling Accuracy: 0.905\n",
      "Epoch: 880. Loss: 0.0009119567694142461. Rolling Accuracy: 0.91\n",
      "Epoch: 881. Loss: 0.0009713344625197351. Rolling Accuracy: 0.905\n",
      "Epoch: 882. Loss: 0.0007431864505633712. Rolling Accuracy: 0.901\n",
      "Epoch: 883. Loss: 0.0006246557459235191. Rolling Accuracy: 0.901\n",
      "Epoch: 884. Loss: 0.0005997390253469348. Rolling Accuracy: 0.897\n",
      "Epoch: 885. Loss: 0.0005924654542468488. Rolling Accuracy: 0.897\n",
      "Epoch: 886. Loss: 0.0006850130157545209. Rolling Accuracy: 0.897\n",
      "Epoch: 887. Loss: 0.0007504024542868137. Rolling Accuracy: 0.897\n",
      "Epoch: 888. Loss: 0.0006733966874890029. Rolling Accuracy: 0.897\n",
      "Epoch: 889. Loss: 0.0005511731724254787. Rolling Accuracy: 0.897\n",
      "Epoch: 890. Loss: 0.00045377755304798484. Rolling Accuracy: 0.897\n",
      "Epoch: 891. Loss: 0.0015764907002449036. Rolling Accuracy: 0.897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 892. Loss: 0.005475573241710663. Rolling Accuracy: 0.897\n",
      "Epoch: 893. Loss: 0.005630459636449814. Rolling Accuracy: 0.897\n",
      "Epoch: 894. Loss: 0.0036775260232388973. Rolling Accuracy: 0.897\n",
      "Epoch: 895. Loss: 0.0024385633878409863. Rolling Accuracy: 0.897\n",
      "Epoch: 896. Loss: 0.0019204235868528485. Rolling Accuracy: 0.901\n",
      "Epoch: 897. Loss: 0.0013921783538535237. Rolling Accuracy: 0.905\n",
      "Epoch: 898. Loss: 0.0009997597662732005. Rolling Accuracy: 0.91\n",
      "Epoch: 899. Loss: 0.0013093978632241488. Rolling Accuracy: 0.91\n",
      "Epoch: 900. Loss: 0.002462293254211545. Rolling Accuracy: 0.905\n",
      "Epoch: 901. Loss: 0.0026146459858864546. Rolling Accuracy: 0.901\n",
      "Epoch: 902. Loss: 0.0020729186944663525. Rolling Accuracy: 0.897\n",
      "Epoch: 903. Loss: 0.0016028985846787691. Rolling Accuracy: 0.897\n",
      "Epoch: 904. Loss: 0.0011649971129372716. Rolling Accuracy: 0.897\n",
      "Epoch: 905. Loss: 0.0008536564419046044. Rolling Accuracy: 0.897\n",
      "Epoch: 906. Loss: 0.0006923811160959303. Rolling Accuracy: 0.897\n",
      "Epoch: 907. Loss: 0.0005823411047458649. Rolling Accuracy: 0.897\n",
      "Epoch: 908. Loss: 0.000504264491610229. Rolling Accuracy: 0.897\n",
      "Epoch: 909. Loss: 0.00048792941379360855. Rolling Accuracy: 0.897\n",
      "Epoch: 910. Loss: 0.000539040716830641. Rolling Accuracy: 0.897\n",
      "Epoch: 911. Loss: 0.000544594950042665. Rolling Accuracy: 0.897\n",
      "Epoch: 912. Loss: 0.0005172972450964153. Rolling Accuracy: 0.897\n",
      "Epoch: 913. Loss: 0.0004781231109518558. Rolling Accuracy: 0.897\n",
      "Epoch: 914. Loss: 0.0004432006389833987. Rolling Accuracy: 0.897\n",
      "Epoch: 915. Loss: 0.0004175661888439208. Rolling Accuracy: 0.897\n",
      "Epoch: 916. Loss: 0.00039832605398260057. Rolling Accuracy: 0.897\n",
      "Epoch: 917. Loss: 0.00038280367152765393. Rolling Accuracy: 0.901\n",
      "Epoch: 918. Loss: 0.0003705644921865314. Rolling Accuracy: 0.905\n",
      "Epoch: 919. Loss: 0.0003654455940704793. Rolling Accuracy: 0.905\n",
      "Epoch: 920. Loss: 0.0003757184895221144. Rolling Accuracy: 0.905\n",
      "Epoch: 921. Loss: 0.00036967216874472797. Rolling Accuracy: 0.901\n",
      "Epoch: 922. Loss: 0.00035728412331081927. Rolling Accuracy: 0.897\n",
      "Epoch: 923. Loss: 0.00034044566564261913. Rolling Accuracy: 0.897\n",
      "Epoch: 924. Loss: 0.0003249507280997932. Rolling Accuracy: 0.897\n",
      "Epoch: 925. Loss: 0.000315831188345328. Rolling Accuracy: 0.897\n",
      "Epoch: 926. Loss: 0.00030691197025589645. Rolling Accuracy: 0.897\n",
      "Epoch: 927. Loss: 0.00030035965028218925. Rolling Accuracy: 0.897\n",
      "Epoch: 928. Loss: 0.00029581040143966675. Rolling Accuracy: 0.897\n",
      "Epoch: 929. Loss: 0.0002909266040660441. Rolling Accuracy: 0.897\n",
      "Epoch: 930. Loss: 0.00028590517467819154. Rolling Accuracy: 0.897\n",
      "Epoch: 931. Loss: 0.0002814214676618576. Rolling Accuracy: 0.897\n",
      "Epoch: 932. Loss: 0.0002776712644845247. Rolling Accuracy: 0.897\n",
      "Epoch: 933. Loss: 0.00027439111727289855. Rolling Accuracy: 0.897\n",
      "Epoch: 934. Loss: 0.0002720168558880687. Rolling Accuracy: 0.897\n",
      "Epoch: 935. Loss: 0.00027083486202172935. Rolling Accuracy: 0.901\n",
      "Epoch: 936. Loss: 0.0002693328133318573. Rolling Accuracy: 0.901\n",
      "Epoch: 937. Loss: 0.0002676895528566092. Rolling Accuracy: 0.901\n",
      "Epoch: 938. Loss: 0.0002662121260073036. Rolling Accuracy: 0.901\n",
      "Epoch: 939. Loss: 0.0002645472704898566. Rolling Accuracy: 0.897\n",
      "Epoch: 940. Loss: 0.00026298093143850565. Rolling Accuracy: 0.897\n",
      "Epoch: 941. Loss: 0.0002616537094581872. Rolling Accuracy: 0.897\n",
      "Epoch: 942. Loss: 0.00026129200705327094. Rolling Accuracy: 0.897\n",
      "Epoch: 943. Loss: 0.0002619601145852357. Rolling Accuracy: 0.897\n",
      "Epoch: 944. Loss: 0.00026507172151468694. Rolling Accuracy: 0.897\n",
      "Epoch: 945. Loss: 0.0002688207314349711. Rolling Accuracy: 0.897\n",
      "Epoch: 946. Loss: 0.000269639102043584. Rolling Accuracy: 0.897\n",
      "Epoch: 947. Loss: 0.00027323889662511647. Rolling Accuracy: 0.892\n",
      "Epoch: 948. Loss: 0.0002916517842095345. Rolling Accuracy: 0.888\n",
      "Epoch: 949. Loss: 0.00029513490153476596. Rolling Accuracy: 0.884\n",
      "Epoch: 950. Loss: 0.0002889258903451264. Rolling Accuracy: 0.879\n",
      "Epoch: 951. Loss: 0.0002741963544394821. Rolling Accuracy: 0.879\n",
      "Epoch: 952. Loss: 0.00027867750031873584. Rolling Accuracy: 0.879\n",
      "Epoch: 953. Loss: 0.0002908324240706861. Rolling Accuracy: 0.884\n",
      "Epoch: 954. Loss: 0.0002782520023174584. Rolling Accuracy: 0.884\n",
      "Epoch: 955. Loss: 0.0002728631370700896. Rolling Accuracy: 0.884\n",
      "Epoch: 956. Loss: 0.0003173947043251246. Rolling Accuracy: 0.888\n",
      "Epoch: 957. Loss: 0.00032930183806456625. Rolling Accuracy: 0.888\n",
      "Epoch: 958. Loss: 0.0003116081061307341. Rolling Accuracy: 0.888\n",
      "Epoch: 959. Loss: 0.0002859518863260746. Rolling Accuracy: 0.888\n",
      "Epoch: 960. Loss: 0.0002652711409609765. Rolling Accuracy: 0.888\n",
      "Epoch: 961. Loss: 0.0002536023675929755. Rolling Accuracy: 0.884\n",
      "Epoch: 962. Loss: 0.0002873590274248272. Rolling Accuracy: 0.888\n",
      "Epoch: 963. Loss: 0.0006860374705865979. Rolling Accuracy: 0.888\n",
      "Epoch: 964. Loss: 0.0009016306139528751. Rolling Accuracy: 0.888\n",
      "Epoch: 965. Loss: 0.0014661175664514303. Rolling Accuracy: 0.897\n",
      "Epoch: 966. Loss: 0.001683052396401763. Rolling Accuracy: 0.892\n",
      "Epoch: 967. Loss: 0.04300401359796524. Rolling Accuracy: 0.897\n",
      "Epoch: 968. Loss: 0.0013869174290448427. Rolling Accuracy: 0.897\n",
      "Epoch: 969. Loss: 0.0013195452047511935. Rolling Accuracy: 0.901\n",
      "Epoch: 970. Loss: 0.0011441659880802035. Rolling Accuracy: 0.91\n",
      "Epoch: 971. Loss: 0.0009660933865234256. Rolling Accuracy: 0.91\n",
      "Epoch: 972. Loss: 0.0007460424094460905. Rolling Accuracy: 0.91\n",
      "Epoch: 973. Loss: 0.0006680134101770818. Rolling Accuracy: 0.905\n",
      "Epoch: 974. Loss: 0.000907093402929604. Rolling Accuracy: 0.901\n",
      "Epoch: 975. Loss: 0.0007750843069516122. Rolling Accuracy: 0.91\n",
      "Epoch: 976. Loss: 0.0005706561496481299. Rolling Accuracy: 0.914\n",
      "Epoch: 977. Loss: 0.0007417883607558906. Rolling Accuracy: 0.91\n",
      "Epoch: 978. Loss: 0.0011736417654901743. Rolling Accuracy: 0.91\n",
      "Epoch: 979. Loss: 0.0013088211417198181. Rolling Accuracy: 0.901\n",
      "Epoch: 980. Loss: 0.0010679374681785703. Rolling Accuracy: 0.897\n",
      "Epoch: 981. Loss: 0.0007581194513477385. Rolling Accuracy: 0.901\n",
      "Epoch: 982. Loss: 0.0005516401724889874. Rolling Accuracy: 0.905\n",
      "Epoch: 983. Loss: 0.00046367099275812507. Rolling Accuracy: 0.91\n",
      "Epoch: 984. Loss: 0.0004245086165610701. Rolling Accuracy: 0.914\n",
      "Epoch: 985. Loss: 0.0004096898774150759. Rolling Accuracy: 0.91\n",
      "Epoch: 986. Loss: 0.0003919256560038775. Rolling Accuracy: 0.906\n",
      "Epoch: 987. Loss: 0.00037574575981125236. Rolling Accuracy: 0.901\n",
      "Epoch: 988. Loss: 0.000355714961187914. Rolling Accuracy: 0.897\n",
      "Epoch: 989. Loss: 0.0003324550634715706. Rolling Accuracy: 0.897\n",
      "Epoch: 990. Loss: 0.00031087451498024166. Rolling Accuracy: 0.897\n",
      "Epoch: 991. Loss: 0.00029611546779051423. Rolling Accuracy: 0.897\n",
      "Epoch: 992. Loss: 0.00028310055495239794. Rolling Accuracy: 0.901\n",
      "Epoch: 993. Loss: 0.0002690326073206961. Rolling Accuracy: 0.901\n",
      "Epoch: 994. Loss: 0.00028670261963270605. Rolling Accuracy: 0.897\n",
      "Epoch: 995. Loss: 0.0003035892150364816. Rolling Accuracy: 0.892\n",
      "Epoch: 996. Loss: 0.0003043513570446521. Rolling Accuracy: 0.884\n",
      "Epoch: 997. Loss: 0.00028960328199900687. Rolling Accuracy: 0.879\n",
      "Epoch: 998. Loss: 0.0002723363577388227. Rolling Accuracy: 0.879\n",
      "Epoch: 999. Loss: 0.00025523113436065614. Rolling Accuracy: 0.883\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_NEW_MODEL:\n",
    "\n",
    "    # instantiate our model, initial optimizer, and loss function\n",
    "    wfm = WildfireModel()\n",
    "    optimizer = optim.AdamW(wfm.parameters(), lr=1e-4)\n",
    "    loss_function = nn.BCELoss()\n",
    "\n",
    "    # declare constants controlling the training process\n",
    "    BATCH_SIZE = 100\n",
    "    EPOCHS = 1000\n",
    "    ROLLING_ACCURACY_SIZE = 4\n",
    "\n",
    "    # instantiate our bounded buffer to keep track of the last ROLLING_ACCURACY_SIZE epochh accuracies\n",
    "    past_epoch_accuracies = BoundedNumericList(ROLLING_ACCURACY_SIZE)\n",
    "    highest_rolling_accuracy = -1\n",
    "\n",
    "    for epoch in range(EPOCHS): # loop over all of our data EPOCH times\n",
    "        for i in range(0, len(train_x), BATCH_SIZE): # iterate over our batches\n",
    "            # grab the ith batch\n",
    "            batch_x = train_x[i : i+BATCH_SIZE]\n",
    "            batch_y = train_y[i : i+BATCH_SIZE]\n",
    "            batch_y = torch.unsqueeze(batch_y, 1)\n",
    "\n",
    "            # zero our gradient\n",
    "            wfm.zero_grad()\n",
    "\n",
    "            # pass the batch through the model\n",
    "            outputs = wfm(batch_x)\n",
    "            \n",
    "            # compute the loss between the outputs and the expected\n",
    "            loss = loss_function(outputs, batch_y)\n",
    "            \n",
    "            # update the model's weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # calculate the most recent epoch's accuracy\n",
    "        epoch_accuracy,_,_ = evaluate_accuracy(test_x, test_y, wfm)\n",
    "        \n",
    "        # add it to the list of past accuracies\n",
    "        past_epoch_accuracies.insert(epoch_accuracy)\n",
    "        \n",
    "        # calculate the rolling average\n",
    "        rolling_accuracy = past_epoch_accuracies.average()\n",
    "\n",
    "        print(f\"Epoch: {epoch}. Loss: {loss}. Rolling Accuracy: {round(rolling_accuracy,3)}\")\n",
    "\n",
    "        # save the model if the most recent updates have been beneficial\n",
    "        if rolling_accuracy > highest_rolling_accuracy:\n",
    "            print(f\"Saving model at epoch {epoch}.\")\n",
    "            torch.save(wfm, \"wfm.pt\")\n",
    "            highest_rolling_accuracy = rolling_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on test data: 0.948\n",
      "2 'fires' identified as 'no fires' on test data.\n",
      "1 'no fires' identified as 'fires' on test data.\n",
      "Model accuracy on extra data: 0.899\n",
      "0 'fires' identified as 'no fires' on extra data.\n",
      "42 'no fires' identified as 'fires' on extra data.\n"
     ]
    }
   ],
   "source": [
    "wfm = torch.load('wfm.pt')\n",
    "\n",
    "acc,correct,incorrect = evaluate_accuracy(test_x, test_y, wfm)\n",
    "incorrect_fire = int(sum(test_y[incorrect])) # fires incorrectly identified as no fire\n",
    "incorrect_no_fire = len(test_y[incorrect]) - incorrect_fire # no fires incorrectly identified as fire\n",
    "\n",
    "print(f\"Model accuracy on test data: {acc}\")\n",
    "print(f\"{incorrect_fire} 'fires' identified as 'no fires' on test data.\")\n",
    "print(f\"{incorrect_no_fire} 'no fires' identified as 'fires' on test data.\")\n",
    "\n",
    "extra_acc,extra_correct,extra_incorrect = evaluate_accuracy(extra_x, extra_y, wfm)\n",
    "extra_incorrect_fire = int(sum(extra_y[extra_incorrect])) # fires incorrectly identified as no fire\n",
    "extra_incorrect_no_fire = len(extra_y[extra_incorrect]) - extra_incorrect_fire # no fires incorrectly identified as fire\n",
    "\n",
    "print(f\"Model accuracy on extra data: {extra_acc}\")\n",
    "print(f\"{extra_incorrect_fire} 'fires' identified as 'no fires' on extra data.\")\n",
    "print(f\"{extra_incorrect_no_fire} 'no fires' identified as 'fires' on extra data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
